<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>如何成为一个有产品思维的研发人员</title>
    <link href="/longblog/posts/22_07_16_20_12_how_to_be_a_developer_with_product_sense.html"/>
    <url>/longblog/posts/22_07_16_20_12_how_to_be_a_developer_with_product_sense.html</url>
    
    <content type="html"><![CDATA[<p>#问题 #产品 #思维</p><p>如何成为一个有产品思维的研发</p><ol><li> 产品的思维是什么样的？</li><li> 什么叫做用户思维？</li><li> 如何拥有同理心和洞察力？</li><li> 产品的职业技能有哪些？</li><li> 怎么把领域模型的思路用在产品思维上？</li><li> 怎么把 uml 的技能用在产品技能上？</li><li> 怎么借鉴产品经理的推动力(项目经理的推动力)？</li><li> 怎么借鉴产品经理的沟通能力？</li></ol><hr><blockquote><p>Friendship without self-interest is one of the rare and beautiful things of life.<br>— <cite>James F. Byrnes</cite></p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>质量保证梳理</title>
    <link href="/longblog/posts/22_07_16_20_02_some_thing_about_qa.html"/>
    <url>/longblog/posts/22_07_16_20_02_some_thing_about_qa.html</url>
    
    <content type="html"><![CDATA[<p>#qa #quality #项目工程 #质量 #质量保证 </p><p>对于一些需求的实现，使用checklist的方式，回答清楚每一个跟质量相关的问题，帮助开发人员梳理清楚对于质量保证而言最重要的事是什么，做到质量保证“能落地”。</p><h3 id="问题列表："><a href="#问题列表：" class="headerlink" title="问题列表："></a>问题列表：</h3><h4 id="如何保证质量？"><a href="#如何保证质量？" class="headerlink" title="如何保证质量？"></a>如何保证质量？</h4><p>首先得定义质量，要能对质量进行衡量</p><p>对于一个软件项目而言，质量意味着什么</p><h5 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h5><ul><li>以当前的资源部署下，在业务上，能支撑多少业务量(tps/qps)？</li><li>最早遇到性能瓶颈的模块是什么 ？</li><li>最简单能解决该性能瓶颈的方法是什么，解决成本怎么样？</li><li>从设计上，需要达到20-100倍的设计标准，当前的方案能否达到目标？</li><li>扩容/缩容的成本如何？</li></ul><h5 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h5><ul><li>当前设计是否存在单点故障？</li><li>极端情况下，是否能够正常提供共服务(流量突增、恶意刷接口)？</li><li>节点故障是否会影响到正常使用(部分节点宕机)？</li><li>功能是否具备feature toggle？</li><li>功能是否具备回滚能力？</li><li>若发生错误，是否能第一时间得到通知？</li><li>若发生错误，是否任何人都能快速定位到问题？</li></ul><h5 id="可维护性"><a href="#可维护性" class="headerlink" title="可维护性"></a>可维护性</h5><ul><li>预计可能的功能拓展有些什么？可能的功能需要增加哪些内容？</li><li>是否是插件化设计，组件的替换成本如何？</li><li>对于较大的功能模块，是否有比较明显的设计模式说明？</li><li>功能是否具有两个以上的人熟悉，做到维护人员冗余？</li></ul><h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><ul><li>是否有详细的说明文档、设计文档、沟通文档、项目过程文档？</li><li>是否有针对于项目结构/模块设计/代码风格与他人进行沟通讨论？</li><li>是否有完善的单元测试及集成测试？</li><li>是否进行过技术探讨与反思？例如当前项目的主要问题是什么？解决的优先级是怎样的？当前的解决方案是否具有通用性？能否有所沉淀以成为通用组件？等</li></ul><h4 id="如何用测试来保障质量？"><a href="#如何用测试来保障质量？" class="headerlink" title="如何用测试来保障质量？"></a>如何用测试来保障质量？</h4><p>测试 是更接近实际的一种质量保证手段。</p><h5 id="2-1-如何保证功能的正确性？"><a href="#2-1-如何保证功能的正确性？" class="headerlink" title="2.1 如何保证功能的正确性？"></a>2.1 如何保证功能的正确性？</h5><h6 id="单元测试-集成测试"><a href="#单元测试-集成测试" class="headerlink" title="单元测试/集成测试"></a>单元测试/集成测试</h6><ul><li>如何保证(自动化)单元测试的覆盖率</li><li>学习QA如何写测试用例</li><li>做测试的代码覆盖率检查<h6 id="如何能快速管理集成测试"><a href="#如何能快速管理集成测试" class="headerlink" title="如何能快速管理集成测试"></a>如何能快速管理集成测试</h6></li><li>super_test 项目<h6 id="如何保证集成测试的场景覆盖率"><a href="#如何保证集成测试的场景覆盖率" class="headerlink" title="如何保证集成测试的场景覆盖率"></a>如何保证集成测试的场景覆盖率</h6></li><li>学习QA如何写测试用例</li><li>先以一系列场景为例，实际写一堆用例再来回答这个问题</li></ul><h5 id="2-2-如何保证性能和可扩展性？"><a href="#2-2-如何保证性能和可扩展性？" class="headerlink" title="2.2 如何保证性能和可扩展性？"></a>2.2 如何保证性能和可扩展性？</h5><h6 id="性能测试、故障演练-性能瓶颈"><a href="#性能测试、故障演练-性能瓶颈" class="headerlink" title="性能测试、故障演练(性能瓶颈)"></a>性能测试、故障演练(性能瓶颈)</h6><p>性能测试主要为 <code>单模块性能测试</code> 和 <code>链路集成测试</code></p><p>对于模块的测试，可以先站在基础架构的角度，考虑模块的可扩展性，再站在实际使用的角度，从业务角度测试性能。</p><p>例如，对于messenger的压测，可以先考虑当前使用的 redis、mongo、kafka 的本身的拓展能力，得到在不同流量级别下这些模块的性能。</p><p>然后，考虑 messenger 目前提供的服务内容，针对每个业务内容，进行单独的测试，类似于单接口测试。然后对多接口进行测试，考虑不同接口之间的相互影响因素。</p><p>对于链路压测，需要考虑 1. 链路范围界定 2. mock服务。(ps: 可以考虑mock为正常服务的一个切面)</p><h5 id="2-3-自动化性能测试平台的构想"><a href="#2-3-自动化性能测试平台的构想" class="headerlink" title="2.3 自动化性能测试平台的构想"></a>2.3 自动化性能测试平台的构想</h5><p>对于业务的性能测试，压测的情况越接近于真实业务情况的压测约具有说服力。因此，性能测试是要尽量模拟真实的业务场景。</p><p>但对于一个变化较快的业务，或者一个新的业务，是很难准确估计真实的业务流量模型的。</p><p>这种时候最好的方式是对多个可能的变量进行笛卡尔积，然后对每种情况进行压测。</p><p>举个例子，messenger可能面临的业务流量模型有很多，其中 总连接数、单个房间人数、房间个数、首页关注人数、changeset发送频率、changeset大小、广播消息类型/频率 等都是可能的变量</p><p>例如，平均一个房间10个人，单实例上 100 个房间，总连接5000时，用户以 6帧/s 的速度发送 800byte 大小的changeset，那么，对于资源的耗用情况如何？</p><p>如果要对每一种情况进行手工压测，那么其成本是巨大的，</p><p>从 do not repeat yourself 的角度出发，这应该是一个可以自动化的流程。</p><p>对于压测人员，需要做的是： 1. 找出约束的变量，配置关注的变量值。 2. 对结果进行分析并做近一步的确认。</p><p>自动化性能测试平台最好是一个可以适应多种场景的通用化平台，支持自定义的压测脚本、自定义的压测报告、自定义的压测策略，</p><p>应该支持多种消息格式，例如 http、https、ws、wss、grpc、mqtt 等。</p><p>对于这个平台的搭建，一定要仔细去分析 jmeter、metersphere 这两个平台的逻辑以及设计。</p><h5 id="2-4-如何保证可靠性？"><a href="#2-4-如何保证可靠性？" class="headerlink" title="2.4 如何保证可靠性？"></a>2.4 如何保证可靠性？</h5><ul><li>  边界测试、极端测试、混沌测试、故障演练(错误故障)</li></ul><h6 id="可以考虑以下内容："><a href="#可以考虑以下内容：" class="headerlink" title="可以考虑以下内容："></a>可以考虑以下内容：</h6><ul><li>  api的边界测试交给中间件来进行。</li><li>  梳理可能的错误点，并考虑故障注入的方式。</li><li>  跳出代码逻辑去考虑错误故障，目标是得到尽可能全的故障类型，并针对每个可能的故障进行故障排查及恢复演练。</li></ul><h6 id="当前最紧要的事："><a href="#当前最紧要的事：" class="headerlink" title="当前最紧要的事："></a>当前最紧要的事：</h6><ul><li>  故障演练的规范化。</li></ul><h5 id="2-5-如何保证用户角度的正确性？"><a href="#2-5-如何保证用户角度的正确性？" class="headerlink" title="2.5 如何保证用户角度的正确性？"></a>2.5 如何保证用户角度的正确性？</h5><ul><li>  视觉感知测试/e2e测试</li></ul><p>e2e测试的成本相对较高，需要测试人员写相应的代码，且每当业务逻辑产生变化，则需要维护这些用例代码。</p><p>视觉感知测试是另一种和e2e测试类似的测试，一般来说不需要写代码，但由于测试是基于“图片对比”的，因此灵活性不如 e2e,</p><p>视觉感知测试的优点是“直观”，在考虑操作录制的情况下，测试人员仅需要对“用例场景”进行定义即可，要求相对较低。</p><h4 id="视觉感知测试怎么做？"><a href="#视觉感知测试怎么做？" class="headerlink" title="视觉感知测试怎么做？"></a>视觉感知测试怎么做？</h4><p>最基础的方式，是进行图片对比，有一系列的操作逻辑，每一步操作后，都有一个截图，用于展示当前操作状态。</p><p>只要一个功能的逻辑没有改动，则特定操作会产生特定的结果。</p><p>只要重复这样的操作逻辑，并对每次的图片进行比对，就能得出特定的结论。</p><p>划分应当以 功能 区分开。</p><p>在某一个功能下，有一些具体的场景。</p><p>每个场景都有特定的操作与逻辑。</p><p>很多场景是相关联的，这些场景应当可以进行参数化配置。</p><p>场景中的很多流程是相似的，这些流程应当可以复用。</p><p>由于运行与截图需要对测试人员透明，因此需要提供录制场景的方法。</p><p>考虑到功能可能频繁变更，因此要增加快速处理比对失败的场景的能力。</p><p>考虑到测试的后台运行属性，因此需要具备报警与通知的功能。</p><p>考虑到素材的可变更性，需要标注可变素材块。</p><p>考虑到h5的布局对内容的影响，可以考虑对于布局的视觉感知测试。</p><h6 id="当前最紧要的事：-1"><a href="#当前最紧要的事：-1" class="headerlink" title="当前最紧要的事："></a>当前最紧要的事：</h6><p>以一个功能需求为实际场景，进行MVP实验</p><h4 id="考虑当前-master-项目的整体质量保证措施"><a href="#考虑当前-master-项目的整体质量保证措施" class="headerlink" title="考虑当前 master 项目的整体质量保证措施"></a>考虑当前 master 项目的整体质量保证措施</h4><ul><li>  以瑞阳牵头的发布流程的保证</li><li>  各小组进行的code review</li><li>  写轮眼项目</li><li>  前端项目的e2e模块</li><li>  后端的api接口自动化测试</li><li>  监控报警</li><li>  基于用户反馈的 oncall 机制</li></ul><p>文档直通车：</p><p><a href="/longblog/posts/22_07_16_20_00_something_about_qa.html" name="关于质量保证的探讨" >关于质量保证的探讨</a></p><hr><blockquote><p>The greatest achievement of humanity is not its works of art, science, or technology, but the recognition of its own dysfunction.<br>— <cite>Eckhart Tolle</cite></p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>关于质量保证的探讨</title>
    <link href="/longblog/posts/22_07_16_20_00_something_about_qa.html"/>
    <url>/longblog/posts/22_07_16_20_00_something_about_qa.html</url>
    
    <content type="html"><![CDATA[<p>#qa #quality #项目工程 #质量 #质量保证</p><h2 id="什么是质量？"><a href="#什么是质量？" class="headerlink" title="什么是质量？"></a>什么是质量？</h2><p>系统质量，首先保证系统的正常运行，包括功能正确、系统稳定性。其次，要保证系统的健壮性，包括系统的容错能力、报警能力、自恢复能力。还要保证系统的可维护性和可扩展性，包括系统功能增加/修改的难易程度、系统横向扩展的能力。</p><h2 id="我们主要关注哪些点？"><a href="#我们主要关注哪些点？" class="headerlink" title="我们主要关注哪些点？"></a>我们主要关注哪些点？</h2><ol><li> 保证正确的业务逻辑</li></ol><p>任意一个功能都是符合业务预期的，覆盖业务所需要的所有场景。</p><ol start="2"><li> 保证可维护性</li></ol><p>考虑功能规划，确保在之后的新业务需求开发时，能流程地接入系统，而不是做trick兼容。</p><p>新老功能的替换时，确保在数据转换上的可实施。</p><p>在新老功能替换时，确保平滑迁移。</p><ol start="3"><li> 保证容错性(健壮性)</li></ol><p>确保程序有充分的错误处理机制，不会出现程序崩溃宕机的情况。</p><p>确保有明确的监控指标和告警措施，能在程序出现问题时第一时间感知。</p><p>确保有明确的应急方案，例如流量切换，降级、熔断，feature toggle等方案</p><ol start="4"><li> 保证可扩展性</li></ol><p>程序的设计尽量是无状态的，可以水平扩展的。</p><p>如果无法做到无状态，就要做到分治，最终达到可以无限扩展的能力。</p><ol start="5"><li> 保证性能</li></ol><p>选择合适的处理流程，能够异步处理的尽量异步，尽量保证主流程上没有较大的阻塞操作。</p><p>尽量保证程序逻辑层面的不冗余，再保证代码结构设计层面的性能优化。</p><p>由于数据库往往是最终瓶颈，因此要始终保持sql的高效，主要是 分页、批量、索引、独立 几个方式。</p><ol start="6"><li> 安全性？</li></ol><h2 id="如何保证："><a href="#如何保证：" class="headerlink" title="如何保证："></a>如何保证：</h2><ol><li> 代码规范(命令规范、职责单一、分层清晰等)</li><li> 写代码时的高标准(设计模式、面向对象、依赖注入、结构清晰)</li><li> 复杂问题的方案设计与review</li><li> 代码质量保证工具(如sonar、go vet)</li><li> 测试(单元测试、接口测试、集成测试、e2e测试、性能测试)</li><li> 不断重构(小步持续性重构)</li></ol><h2 id="如何执行："><a href="#如何执行：" class="headerlink" title="如何执行："></a>如何执行：</h2><ol><li> [代码规范] =&gt; 有规范、有review</li><li> [高要求] =&gt; 梳理流程、整理文档、先设计再执行、拉人探讨</li><li> [方案设计与review] =&gt; 有文档、有review流程</li><li> [代码质量工具] =&gt; CI/CD 步骤</li><li> [不断重构] =&gt; 每个月自查代码问题</li><li> [测试] =&gt; 见下方</li></ol><h4 id="单元测试的要求"><a href="#单元测试的要求" class="headerlink" title="单元测试的要求"></a>单元测试的要求</h4><ol><li> 所有库方法，必须有正确性单元测试用例。</li><li> 复杂的库方法，需要有多种边界情况测试用例。</li><li> 单元测试用例管理直接在当前模块下的 xxx_test.go 文件中。</li></ol><h4 id="接口测试要求"><a href="#接口测试要求" class="headerlink" title="接口测试要求"></a>接口测试要求</h4><p>目的：保证单个接口的输入输出的正确性</p><p>主要内容： 1. 输入值的边界参数传入。 2. 确保输出值的schema。</p><h4 id="集成测试要求"><a href="#集成测试要求" class="headerlink" title="集成测试要求"></a>集成测试要求</h4><p>目的：从用户行为的角度，保证功能的正确性。</p><p>主要内容：</p><ol><li> 从场景出发，进行数据准备。</li><li> 实际测试，对返回值进行断言。</li></ol><h4 id="e2e测试要求"><a href="#e2e测试要求" class="headerlink" title="e2e测试要求"></a>e2e测试要求</h4><ol><li> 重要流程的场景测试，例如，登录、注册、创建team、创建project、打开文档等。</li><li> 可以考虑基于图片对比的测试。(视觉感知测试)</li></ol><h4 id="性能测试要求"><a href="#性能测试要求" class="headerlink" title="性能测试要求"></a>性能测试要求</h4><ol><li> 单服务性能测试</li></ol><ul><li>  将所有服务拆分，对其进行性能测试。</li><li>  将服务的依赖项进行拆分，mock。</li></ul><ol start="2"><li> 集成服务性能测试</li></ol><ul><li>  全链路数据压测，数据准备与压测。</li><li>  数据来源可采用流量录制。</li></ul><ol start="3"><li> 数据库性能测试</li></ol><ul><li>  对业务场景进行数据库操作过程排查</li><li>  对单个sql语句进行查询分析</li></ul><h2 id="计划"><a href="#计划" class="headerlink" title="计划"></a>计划</h2><p>服务质量的保证主要应当从</p><ol><li> 团队开发人员意识培养</li><li> 团队质量保证规范与流程</li><li> 测试</li></ol><p>这三个方面入手。</p><p>我当前从 测试 入手，主要有以下几个方面的内容：</p><h4 id="完成-api-接口测试框架搭建与开发"><a href="#完成-api-接口测试框架搭建与开发" class="headerlink" title="完成 api 接口测试框架搭建与开发"></a>完成 api 接口测试框架搭建与开发</h4><p>super-test 项目，目标是让团队成员写 <code>接口测试</code> 的难度降低，现在已经做到：</p><ol><li> 使用 json 配置文件进行测试</li><li> 具有简单的测试触发页面</li></ol><h4 id="考虑-websocket-的功能测试"><a href="#考虑-websocket-的功能测试" class="headerlink" title="考虑 websocket 的功能测试"></a>考虑 websocket 的功能测试</h4><p>[TODO]</p><h4 id="考虑-性能测试-框架"><a href="#考虑-性能测试-框架" class="headerlink" title="考虑 性能测试 框架"></a>考虑 性能测试 框架</h4><ol><li> 项目性能</li><li> 中间件性能</li><li> 数据库性能</li></ol><p>文档直通车：<br><a href="/longblog/posts/22_07_16_20_02_some_thing_about_qa.html" name="质量保证梳理" >质量保证梳理</a></p><hr><blockquote><p>A man’s growth is seen in the successive choirs of his friends<br>— <cite>Ralph Waldo Emerson</cite></p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>模块设计优劣的评判维度</title>
    <link href="/longblog/posts/2207161954.html"/>
    <url>/longblog/posts/2207161954.html</url>
    
    <content type="html"><![CDATA[<p>#设计 #设计原则  #设计模式 #架构 #架构评价</p><p>一些技术观点 (模块评判优劣)</p><h3 id="如何评判一个模块的优劣？"><a href="#如何评判一个模块的优劣？" class="headerlink" title="如何评判一个模块的优劣？"></a>如何评判一个模块的优劣？</h3><ol><li> 功能维度：</li></ol><ul><li>评判标准有三个：<ul><li>  可复用</li><li>  可扩展</li><li>  可维护</li></ul></li><li>落地原则有：<ul><li>  基于抽象编程，而非基于实现编程</li><li>  良好的接口设计 (命名规范、善用模式、预留扩展口)</li><li>  完善的测试用例</li><li>  DRY、KISS、YAGNI、SOLID</li></ul></li></ul><ol start="2"><li> 技术维度</li></ol><ul><li>判断标准有四个：<ul><li>  高并发</li><li>  高性能</li><li>  高可靠</li><li>  伸缩性</li><li>  数据一致性</li></ul></li><li>落地需要考虑的问题：<ul><li>  CAP 的选择</li></ul></li></ul><h3 id="技术和业务的关系？"><a href="#技术和业务的关系？" class="headerlink" title="技术和业务的关系？"></a>技术和业务的关系？</h3><ol><li> 技术需要为业务服务，抛开业务谈技术，在有明确商业目标的企业中，没有意义。</li><li> 技术方案的选择不是 “业务决定的”，经常是权衡利弊之后，一起做出的决策。</li></ol><hr><blockquote><p>Love is composed of a single soul inhabiting two bodies.<br>— <cite>Aristotle</cite></p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>groupcache 源码阅读记录</title>
    <link href="/longblog/posts/2207161947.html"/>
    <url>/longblog/posts/2207161947.html</url>
    
    <content type="html"><![CDATA[<p>#缓存 #cache #groupcache #readingcodes</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>“缓存” 是一个 非常、非常、非常 通用的模块，是提升 程序性能/系统性能 的极佳方式之一，缓存的模块有非常多的开源方案。</p><p>本地缓存可以使用 gcache、goframe中也有一个gcache，自己要实现一个简单的 LRU 的缓存也不难。</p><p>分布式缓存中，最常用的是 redis，几乎通吃，以前也有一些团队使用 memcache，现在几乎被 redis 替代。</p><p>内存中的缓存还可以看下 godis 这个项目，golang 的 redis 实现。</p><p>而 groupcache ，作为内存中的 分布式缓存，实现非常简洁，很有参考价值。值得一提的是，groupcache 的作者就是 memcache 的作者。</p><h2 id="可参考的方面"><a href="#可参考的方面" class="headerlink" title="可参考的方面"></a>可参考的方面</h2><h3 id="Pb-和-httpv2-提升性能"><a href="#Pb-和-httpv2-提升性能" class="headerlink" title="Pb 和 httpv2 提升性能"></a>Pb 和 httpv2 提升性能</h3><p>常规我们接触 pb，都是直接使用 grpc 生成代码，使用 grpc 的框架提供 rpc 服务，但是在 groupcache 中，作者直接把 pb 拿出来使用，并且结合 httpv2 的连接复用，也同样能够提供高性能，一定程度上达到了合 grpc 一样的能力。</p><h3 id="Singleflight-去除重复请求"><a href="#Singleflight-去除重复请求" class="headerlink" title="Singleflight 去除重复请求"></a>Singleflight 去除重复请求</h3><p>通用的解决重复请求的方案，常用于解决缓存穿透问题。</p><h3 id="87行代码的-lru-策略"><a href="#87行代码的-lru-策略" class="headerlink" title="87行代码的 lru 策略"></a>87行代码的 lru 策略</h3><p>一个双向链表 + 一个 hashmap，前者用来做 lru，后者用来做缓存。</p><p>这个双向链表是由 golang 提供的，初次之外，golang官方库还提供了 ring 和 heap 两种数据结构。</p><h3 id="48行代码的一致性hash"><a href="#48行代码的一致性hash" class="headerlink" title="48行代码的一致性hash"></a>48行代码的一致性hash</h3><p>有多个 hash slot，无删除 slot 操作。</p><p>使用的 []int 的方式做的 slot，这种方式在查询时的时间复杂度是 Ologn，这种方式和 redis client 的 slot 实现方式不一样，redis 用了更多的空间，但得到的是 O1 的查询复杂度。</p><h2 id="一些特点"><a href="#一些特点" class="headerlink" title="一些特点"></a>一些特点</h2><ol><li> 会做本地缓存(hot cache)，且无更新机制</li><li> 有 load 机制，本地缓存没有 且 未能从 peer 获取时，进行 load</li><li> 有 evicted 机制，可以设置回调</li></ol><p>整体来看，和 gcache 这类本地缓存差别不大，加的 peers 可以帮助减少 load 机制的使用，对于 load 操作很重的缓存比较有利。但本地的 cache 无法主动更新的特点可能导致缓存不一致问题。</p><p>生产使用的场景不太确定，可能适用于数据一致性不太重要、load过程很繁重的情况。</p><p>从开发角度来看，这个项目有一些可参考性，例如 48行代码实现一致性hash、87行代码实现 lru、singleflight解决缓存穿透、直接使用 pb 做为序列化方式提升性能、服务间的http使用 http2 提升性能。</p><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul><li><input disabled="" type="checkbox"> 可以再看一下 go-redis 中的 hash slot 保持方法，以及 mongos 如何保证 路由到正确节点。</li></ul><hr><blockquote><p>They must often change, who would be constant in happiness or wisdom.<br>— <cite>Confucius</cite></p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>记一次服务性能的调优排查</title>
    <link href="/longblog/posts/22_07_16_a_record_of_service_performance_improve.html"/>
    <url>/longblog/posts/22_07_16_a_record_of_service_performance_improve.html</url>
    
    <content type="html"><![CDATA[<h3 id="事情起因："><a href="#事情起因：" class="headerlink" title="事情起因："></a>事情起因：</h3><ol><li><p> 某一天，听到浩兄说我们有个告警，是因为 event-tracking 的 cpu 使用超标<br><img src="https://static.longalong.cn/img/warning.png"></p></li><li><p> 我感觉比较奇怪，顺口问了下情况，后来又和 浩兄、窦兄 一起看了下各种监控</p></li></ol><h3 id="查看-cpu、内存、网络-情况"><a href="#查看-cpu、内存、网络-情况" class="headerlink" title="查看 cpu、内存、网络 情况"></a>查看 cpu、内存、网络 情况</h3><p><img src="https://static.longalong.cn/img/asynccode-12.png"></p><p><img src="https://static.longalong.cn/img/asynccode-11.png"></p><p>发现很奇怪，内存和 cpu 是猛增上去的，但网络流量却非常小。</p><p>又听到 浩兄 说当时机器的 socket 连接数被打满了，于是怀疑是 goroutine 的问题，进一步查看 goroutine 监控</p><h3 id="查看-goroutine-监控"><a href="#查看-goroutine-监控" class="headerlink" title="查看 goroutine 监控"></a>查看 goroutine 监控</h3><p><img src="https://static.longalong.cn/img/asynccode-8.png"></p><p>发现确实 goroutine 飙得非常高。 也没多想，怀疑是不是有 for 循环里面起了 协程，导致 goroutine 没控制住？</p><p>于是和 窦兄 一块儿，一通代码查看，……额……，没发现啥问题。</p><p>此事一直耿耿于怀，听相关负责人说，在计划重构 event-tracking ，有点害怕，毕竟私有部署的项目中，我还有些依赖 event-tracking 的 sdk [😱]……</p><p>但由于 ① 没有现场，没法抓各种 profile ② 没有 tracing，不知道究竟卡在哪里了</p><p>毕竟，日志、监控、tracing、pprof 相当于我们的眼睛，没有这些，现在做一切都相当于盲猜……</p><p>为了减轻私有化适配的工作，在浩兄的怂恿下，开始了一番自救操作……</p><h3 id="进一步排查的准备工作"><a href="#进一步排查的准备工作" class="headerlink" title="进一步排查的准备工作"></a>进一步排查的准备工作</h3><ol><li><p> 由于不知道各接口的响应情况，于是初步建了个 metrics 的板子，简单看了下，觉得挖不同接口的响应时长可能没啥意义，这应该是个系统性问题，于是放弃继续做板子，转向其他方向。</p></li><li><p> 由于已经在框架中集成了 tracing 的东西，于是花了半个小时把 event-tracking 的 所有 tracing 接上。(包括 redis、db、kafka、http request)</p></li><li><p> 为了能复现高压力下的场景，在压测机上装了 ab </p></li></ol><h3 id="试下未做调整时的压测情况"><a href="#试下未做调整时的压测情况" class="headerlink" title="试下未做调整时的压测情况"></a>试下未做调整时的压测情况</h3><h4 id="压测结果"><a href="#压测结果" class="headerlink" title="压测结果"></a>压测结果</h4><p>并发 200， qps 54</p><p><img src="https://static.longalong.cn/img/asynccode-2.png"></p><p><img src="https://static.longalong.cn/img/asynccode-10.png"></p><p>tracing </p><p><img src="https://static.longalong.cn/img/20220716105356.png"></p><p>有四个表现奇怪的地方：</p><ol><li><p> 负载非常不均衡，必定有鬼</p></li><li><p> nginx 到 pod 的时间居然达到数秒</p></li><li><p> Pod 内 produce kafka 的时间 居然和 整个请求的时长一样</p></li><li><p> 在 kafka 队列中，居然卡了这么长时间</p></li></ol><h4 id="先怀疑下-kafka-实例的问题"><a href="#先怀疑下-kafka-实例的问题" class="headerlink" title="先怀疑下 kafka 实例的问题"></a>先怀疑下 kafka 实例的问题</h4><p>kafka 监控 </p><p><img src="https://static.longalong.cn/img/asynccode-6.png"></p><p>就这点量，远远达不到 kafka 的瓶颈，跳过。</p><h4 id="怀疑下-是不是代码中用了-同步发送"><a href="#怀疑下-是不是代码中用了-同步发送" class="headerlink" title="怀疑下 是不是代码中用了 同步发送"></a>怀疑下 是不是代码中用了 同步发送</h4><p><img src="https://static.longalong.cn/img/asynccode.png"></p><p>看来并不是，排除。</p><h4 id="怀疑一下-ikafka-包的问题"><a href="#怀疑一下-ikafka-包的问题" class="headerlink" title="怀疑一下 ikafka 包的问题"></a>怀疑一下 ikafka 包的问题</h4><p>此时发现 ikafka 没有接 metrics ，于是看了下之前写的文档，想把 ikafka 的 metrics 接上。</p><p>然后发现 我想接的是 sarama ，但 ikafka 没有暴露 metrics 出来，也没有把 sarama 的 metrics 接口暴露出来，无果……</p><p>这个问题留到之后再怀疑吧</p><h4 id="怀疑一下-kafka-的配置问题"><a href="#怀疑一下-kafka-的配置问题" class="headerlink" title="怀疑一下 kafka 的配置问题"></a>怀疑一下 kafka 的配置问题</h4><p><img src="https://static.longalong.cn/img/asynccode-1.png"></p><p>果然，看到了一个问题，consumer 的 队列数 居然仅设置了 1 ，这岂不是意味着，消息只能一条条从 kafka 取回来？那不得老慢了……</p><p>另外，看到没有开 autocommit，于是也顺手加上。<br><img src="https://static.longalong.cn/img/20220716105438.png"></p><p>ok，这下顺眼多了。其他也不知道咋样，先压一波试下吧。</p><h3 id="调整-channel-size-后的压测"><a href="#调整-channel-size-后的压测" class="headerlink" title="调整 channel size 后的压测"></a>调整 channel size 后的压测</h3><p>刚准备压的，看了一眼 grafana ，懵了…… </p><p><img src="https://static.longalong.cn/img/asynccode-4.png"></p><p>啥情况？？？ Cpu 直接被拉满了？？？</p><p>吓得我反手就抓了一波 pprofile</p><p><img src="https://static.longalong.cn/img/asynccode-9.png"></p><p>发现居然有大量的 park 方法的调用，这显然就是 goroutine 疯狂切换导致的问题啊。</p><p>按以往的经验，很有可能是 for 循环中的 select 不是全阻塞的。此时跑去搜了一圈 <code>for</code> 的代码。没发现问题，每个 for 循环都还比较规范……</p><p>于是回来接着看 pprofile，注意到 左边 kafka 的调用，按理，这是属于底层包的调用啊，应该不会有啥问题啊。</p><p>跟了一圈代码，由于是直接的 cgo 调用，也很难继续追下去了。</p><p><img src="https://static.longalong.cn/img/origin_img_v2_e59df2f5-be93-4e16-8749-3f500e19ab9g.jpg"></p><p>想到之前 confluent 给我留下的奇奇怪怪的印象 (主要是因为黑盒问题)，再加上对 sarama 做过比较仔细的源码阅读，想着既然 confluent 不好调试，换成 sarama 先试试吧……</p><h3 id="切换-sarama-后的压测"><a href="#切换-sarama-后的压测" class="headerlink" title="切换 sarama 后的压测"></a>切换 sarama 后的压测</h3><p>并发 200， qps 940</p><p><img src="https://static.longalong.cn/img/asynccode-3.png"></p><p>grafana 监控</p><p><img src="https://static.longalong.cn/img/asynccode-7.png"></p><p>时间分布合理</p><p><img src="https://static.longalong.cn/img/asynccode-5.png"></p><p>打完   收工 ！</p><h3 id="结局："><a href="#结局：" class="headerlink" title="结局："></a>结局：</h3><ol><li><p> Event-tracking 的性能问题至少算是解了</p></li><li><p> 如果需要的话，可以再去整理下 confluent 的正确打开方式 ( 还是算了…… ，直接用 sarama 或者 kafka-go 不香吗 )</p></li><li><p> 公共包最好还是提供一些统一的 metrics 接口、提供统一的 tracing 设置</p></li><li><p> 对于我们现在大多数的业务场景，1c 的 cpu 支撑个 1000qps 问题是不大的，大家可能需要更新下对性能的感性认识</p></li></ol><hr><blockquote><p>There are two kinds of failures: those who thought and never did, and those who did and never thought.<br>— <cite>Laurence J. Peter</cite></p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>serverless_究竟是何方神圣？</title>
    <link href="/longblog/posts/22_07_11_what_is_serverless.html"/>
    <url>/longblog/posts/22_07_11_what_is_serverless.html</url>
    
    <content type="html"><![CDATA[<h2 id="什么是-serverless"><a href="#什么是-serverless" class="headerlink" title="什么是 serverless"></a>什么是 serverless</h2><p>顾名思义，<code>serverless</code> 就是 <code>server-less</code>，也就是 <code>别话时间去搞服务器</code> =&gt; <code>把精力全都放到业务开发上</code> !</p><p>在传统的开发流程中，一个团队的标准配置是： 前端工程师 * x + 后端工程师 * y + 运维工程师 * z 。 前端写交互，后端写数据逻辑与存储，运维就搞服务部署、灰度、日志、监控等等。每个岗位各司其职，看起来十分完美。</p><p>但有时候也没有那么完美，人越多、职责分的越细，岗位间的鸿沟就越大，沟通的成本在组织内就会急剧增加。 通常的表现就是 会越来越多、参会的人也越拉越多，有时候可能一个简单的 <code>上新服务进行调试</code> 的工作，都需要大动干戈搞十来个人拉会各种同步和对齐。</p><p>这种时候我们可能就会想： 能不能不要这么多麻烦的服务器的问题？由一个统一的平台去解决 网关配置、服务发现、服务调用、日志、告警、数据库、中间件、CICD、负载均衡、动态扩缩容 等等……</p><p>实际上，不论是传统的各种 cmdb，还是各种微服务框架，还是各种服务引擎，都是在为了解决上面列出的这个问题，只是他们选择的方案不同而已。 其中 serverless，是最晚出现的一种方案，是在 微服务架构之后出现的另一种架构风格。</p><p>有一种论调是 serverless = faas + baas。其中 faas 是主计算的，而 baas 是主存储的。实际上我们对服务器的需求本质上就这两类： 计算 + 存储。</p><p>而泛 IT 领域内，大家常说起 serverless 的时候，总喜欢把 AWS 的 lambda 计算作为例子，或者把 阿里云 的 函数计算 作为例子。容易让人误以为 serverless 就是 <code>函数计算</code> 。</p><p>而我认为 serverless ，除了包括 机器视角下 的 faas 以及 baas 外，还包括 开发流 视角下的 serverless 开发工具链 + 管理平台 + 编排系统 + 监控系统。 这是站在 serverless 的终极目标上得出的结论。</p><p>云服务上更关心 机器视角，例如 baas 要包含哪些产品？kv？文档存储？对象存储？table存储？ 例如 faas 有哪些形式的延伸？微服务场景？任务触发场景？</p><p>开发者更关心 开发流视角，例如 如何部署一段程序？如何管理一段程序？如何监控程序运行？</p><h2 id="serverless-解决了什么问题？"><a href="#serverless-解决了什么问题？" class="headerlink" title="serverless 解决了什么问题？"></a>serverless 解决了什么问题？</h2><ol><li>开发和部署工具链</li><li>弹性扩缩容 (按量付费)</li></ol><h2 id="使用-serverless-的场景是什么？"><a href="#使用-serverless-的场景是什么？" class="headerlink" title="使用 serverless 的场景是什么？"></a>使用 serverless 的场景是什么？</h2><ol><li>异步 (事件触发)</li><li>无状态 (少依赖)</li><li>突发性</li></ol><h2 id="serverless-的技术点有哪些？"><a href="#serverless-的技术点有哪些？" class="headerlink" title="serverless 的技术点有哪些？"></a>serverless 的技术点有哪些？</h2><ol><li>如何触发 serverless？(events)</li><li>如何解决弹性扩缩容？<ol><li>基于 kubernetes</li></ol></li><li>如何解决路由绑定？(负载均衡)</li><li>如何解决快速启动？</li><li>如何解决程序运行？(build 过程)</li><li>如何解决服务编排？</li></ol><h2 id="serverless-比较难搞定什么？"><a href="#serverless-比较难搞定什么？" class="headerlink" title="serverless 比较难搞定什么？"></a>serverless 比较难搞定什么？</h2><ol><li><p>状态保持 (本地缓存、长连接)</p></li><li><p>事务</p></li><li><p>编排</p></li><li><p>冷启动</p></li><li><p>黑盒排查问题</p></li><li><p>服务商绑定</p></li></ol><h2 id="阿里云的几款-serverless-产品的异同"><a href="#阿里云的几款-serverless-产品的异同" class="headerlink" title="阿里云的几款 serverless 产品的异同"></a>阿里云的几款 serverless 产品的异同</h2><ol><li>FC</li><li>SAE</li><li>MSE</li><li>ASK</li></ol><p>当我们纵眼观察 阿里云 提供了几类和 serverless 有关的产品时，往往容易犯迷糊。<br>FC 是函数计算，主要目的是提供计算能力，运行的资源粒度可以非常小 (128MB * 0.08 C)，可以运行任意你想要的程序。在使用场景上，类似于 <code>任务</code> 的概念。(当然你也可以把他用于 http server)</p><p>SAE 是 serverless 应用引擎，提供的能力和 FC 类似，都是计算能力。在场景上，类似于 <code>service</code> 的概念，例如启动一个 auth 服务。 相比于传统的自己部署一个服务，SAE 提供了 网关、弹性伸缩、监控 等能力。(其实，MSE 有逐渐替代 SAE 的倾向，毕竟他们的重合度太高了)</p><p>MSE 是 微服务引擎，提供的是 服务治理 的能力。是 注册/配置中心、网关、分布式事务、流量治理、开发测试 的集合体。实际就是把原来 阿里云 提供的各类单个的产品，在 微服务 的应用场景下进行了组合。</p><p>ASK 是 弹性 k8s 集群，也就是 k8s 集群的按量付费版本，提供的是 k8s 基础设施。</p><h2 id="我们对-serverless-抱有什么样的期待？"><a href="#我们对-serverless-抱有什么样的期待？" class="headerlink" title="我们对 serverless 抱有什么样的期待？"></a>我们对 serverless 抱有什么样的期待？</h2><ol><li>完全无运维 <ul><li>一套完善的开发和部署平台</li></ul></li><li>友好一致的开发体验<ul><li>开发者无需关注除 业务需求 之外的一切 (比如 k8s、注册中心、服务发现、CICD 等等)</li></ul></li><li>省钱<ul><li>按需付费，不用为每个不怎么使用的应用都花着钱</li></ul></li></ol><h2 id="一些常见的-serverless-应用"><a href="#一些常见的-serverless-应用" class="headerlink" title="一些常见的 serverless 应用"></a>一些常见的 serverless 应用</h2><ul><li><input disabled="" type="checkbox"> 举一些具体的例子</li></ul><ol><li>音视频行业的转码需求</li><li>设计、图形等领域相关的渲染需求</li><li>推荐系统相关的机器学习需求</li></ol><p>分为 2 类：</p><ol><li>微服务场景</li><li>弹性任务场景</li></ol><h2 id="serverless-平台有哪些？"><a href="#serverless-平台有哪些？" class="headerlink" title="serverless 平台有哪些？"></a>serverless 平台有哪些？</h2><ul><li><input disabled="" type="checkbox"> 对比各家的产品，看看他们都在解决什么问题？</li></ul><ol><li>openfaas</li><li>knative</li><li>kubeless</li><li>阿里云相关产品</li><li>腾讯云相关产品</li><li>AWS 相关产品</li></ol><h2 id="我会怎么选？"><a href="#我会怎么选？" class="headerlink" title="我会怎么选？"></a>我会怎么选？</h2><ul><li><input disabled="" type="checkbox"> 列举一些场景，分别在这些场景下我会如何决策？</li></ul><p>如果我是团队 TL，我会如何选择？</p><h2 id="可以参考的文档"><a href="#可以参考的文档" class="headerlink" title="可以参考的文档"></a>可以参考的文档</h2><ol><li><a href="https://www.aliyun.com/product/aliware/fnf">阿里云 serverless 工作流</a></li><li><a href="https://help.aliyun.com/document_detail/97792.html">阿里云 serverless 应用引擎</a></li><li><a href="https://www.aliyun.com/product/fc">阿里云 FC 函数计算</a></li><li><a href="https://www.aliyun.com/product/cs/ask">阿里云 ASK 容器服务</a></li><li><a href="https://cloud.tencent.com/document/product/583/9199">腾讯云函数</a></li><li><a href="https://cloud.tencent.com/document/product/1154">腾讯云 serverless 应用中心</a></li><li><a href="https://cloud.tencent.com/document/product/1371">腾讯云 弹性微服务</a></li><li><a href="https://firebase.google.com/">firebase</a></li></ol><hr><blockquote><p>Through meditation and by giving full attention to one thing at a time, we can learn to direct attention where we choose.<br>— <cite>Eknath Easwaran</cite></p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>serverless</tag>
      
      <tag>faas</tag>
      
      <tag>baas</tag>
      
      <tag>cloud native</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>service mesh 是如何产生的？</title>
    <link href="/longblog/posts/22_06_16_how_service_mesh_born.html"/>
    <url>/longblog/posts/22_06_16_how_service_mesh_born.html</url>
    
    <content type="html"><![CDATA[<h3 id="演进过程"><a href="#演进过程" class="headerlink" title="演进过程"></a>演进过程</h3><p>最原始的应用，是由小团队直接负责的单体应用，直观的感受是，团队里的每个人，都对整个项目的技术选型具有影响力(姑且认为 每个人都是 “技术管理委员会” 的成员)。由于仅有一个大项目，也就基本不存在 “平台能力” 的概念。</p><p>单体项目逐渐膨胀，分层逐渐模糊、模块逐渐耦合、调用逐渐混乱、发布越发频繁、单个错误的影响面越发增大…… ，这就进入到所谓的 “单体地狱” 阶段。 解决办法之一，就是进行 服务化演进。</p><h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><p>当单体服务拆分成多个服务后，从代码功能上，出现了一系列 与业务功能无关的 相似需求，例如 <code>服务发现</code>、<code>配置管理</code>、<code>负载均衡</code>、<code>健康检查</code>、<code>限流熔断</code>、<code>安全加密</code>、<code>协议转换</code>、<code>认证授权</code>、<code>链路追踪</code>、<code>通用监控</code> 等，我们将之统称为 “平台能力”。</p><p>在组织结构上，不同的服务 会被 不同的团队(人员) 所管理，服务的技术走向 会逐渐脱离原有的 “技术管理委员会” 的管理，逐渐出现 不同代码实现、不同库选型、不同框架选型、不同语言选型 等等情况。</p><p>当出现了一些上述的问题后，有识之士 就会想： <strong>“如何统一提供平台能力，解放业务专家们，让业务同学专注业务开发？”</strong></p><h3 id="解决问题的方向"><a href="#解决问题的方向" class="headerlink" title="解决问题的方向"></a>解决问题的方向</h3><h4 id="进程内方案"><a href="#进程内方案" class="headerlink" title="进程内方案"></a>进程内方案</h4><p>最直接的想法会是：根据不同语言，提供平台能力的 sdk。表现形式可能是： 工具库、应用框架。例如，统一的 web server 框架，在框架中提供 服务发现、负载均衡、监控、限流 等平台能力。</p><p>举个栗子，java 中的 <code>spring cloud</code> 提供了几大组件解决平台能力，如： <code>Eureka</code> 解决服务注册与发现、 <code>Ribbon</code> 解决负载均衡、 <code>Hystrix</code> 解决系统保护、 <code>Gateway</code> 提供 api 网关、 <code>Spring cloud config</code> 解决配置管理……</p><p>实际上，在单个语言体系下的各类 微服务框架 都是在解决这个问题。例如 golang 下的 <code>kratos</code>、<code>go-zero</code>、<code>go-micro</code>、<code>rpcx</code>、<code>kitex</code>、<code>go-kit</code> 等。(每个语言下都有一大堆)</p><p>但这种方式也存在 2 个主要问题： </p><ol><li>如果使用了多个语言体系，那么代码的维护成本就需要 *N 。例如，一个 负载均衡 的能力，如果有 python、java、golang、nodejs 4种语言，就需要对应的开发团队维护 4 套代码。难度之大，难以想象。  这个假设，其实是建立在一些 <code>大型团队</code>、<code>历史包袱重</code> 的前提下的。 对于大多数中小团队，服务端其实也只有一个语言体系，因此不失为一个不错的方案。</li><li>当平台能力不够稳定时(其实几乎很难稳定)，平台能力的升级需要业务代码同步更新，而推动业务同学进行库的升级，在稍大点的团队中是十分费心费力的。这也就是 代码耦合 带来的弊端。</li></ol><h4 id="进程外方案"><a href="#进程外方案" class="headerlink" title="进程外方案"></a>进程外方案</h4><p>在遇到上面的问题后，自然而然的想法，就是将一些平台能力进行抽离，在单独的进程中运行。独立部署后，使用一系列进程间通信的方式提供平台能力，就解决了 “语言绑定” 和 “代码绑定” 的问题。就这样，sidecar 模式诞生了！</p><p>Sidecar 在解决一些问题的同时，自然也引入了一些问题，主要有两个：</p><ol><li>通信方式的问题。</li><li>性能的问题。</li></ol><p>性能问题，从全局来看，由于多了一层进程外处理流程，不可避免地肯定会有性能损耗。解决办法自然就是尽量降低 sidecar 本身处理的资源耗用，包括 语言选型使用更高性能的语言(如 go、c++ 等)、代码层的架构设计和实现优化、扩展点使用更高性能的实现 (例如 grpc、wasm、golang、lua 等)。</p><p>通信方式 有两个走向：</p><ol><li>业务完全透明 的流量劫持 模式。</li><li>提供 http/grpc 的通信模式。</li></ol><h3 id="side-car-一览"><a href="#side-car-一览" class="headerlink" title="side car 一览"></a>side car 一览</h3><p>目前社区中比较出彩的 sidecar 解决平台能力的方案有： <a href="https://konghq.com/kong-mesh">kong(kuma)</a>、<a href="https://github.com/istio/istio">istio(envoy)</a>、<a href="https://github.com/traefik/mesh">traefik-mesh</a>、<a href="https://docs.nginx.com/nginx-service-mesh">nginx mesh (不开源)</a>、<a href="https://github.com/linkerd/linkerd2">linkerd</a>、 <a href="https://github.com/dapr/dapr">dapr</a> 、<a href="https://github.com/mosn/mosn">mosn</a>、<a href="https://github.com/megaease/easegress">easegress</a> 、<a href="https://github.com/hashicorp/consul">consul</a> 、<a href="https://github.com/openservicemesh/osm">osm(envoy)</a></p><p>从上面的项目可以看出，sidecar 大多都是从 <code>网关</code> 延伸而来的，这和 <code>平台能力</code> 的主体 <code>流量治理</code> 是分不开关系的。 基础能力是要做到 <code>服务发现</code>、<code>负载均衡</code>、<code>路由</code>、<code>安全</code>，延伸能力有  <code>限流熔断</code>、<code>可观测性(metrics、tracing)</code> 、<code>流量灰度</code>、<code>故障注入</code>、<code>认证授权</code>、<code>协议转换</code>、<code>管理平台</code> 。 </p><p>所有上面这些项目，有一个比较特殊： <code>dapr</code>，其他的基本都是 <code>网关</code>，而 dapr 被认为是一个 <code>应用运行时</code>，之所以 dapr 也被我列到这里，是因为他们有很多相似的地方，例如：<code>路由</code> 、<code>服务发现</code>、<code>负载均衡</code>、<code>可观测性</code> 等。 </p><p>现在看起来不一样的地方，例如 dapr 提供 <code>kvstore(state)</code>、<code>pubsub</code> 等，目标是提供与平台无关(不太相关) 的特定能力，例如 kvstore 就提供了 <code>DynamoDB</code> 、<code>redis</code>、<code>postgresql</code> 等实现。  从一部分 service mesh 的项目发展方向来看 (envoy、mson等)，流量代理的类型从原有的 <code>tcp</code>、<code>http</code>、<code>ws</code>、<code>http2</code>、<code>grpc</code> 等协议，扩展至 <code>redis</code>、<code>mongodb</code>、<code>dynamodb</code>、<code>kakfa</code>、<code>dubbo</code> 甚至是 自定义 协议(自有 rpc 协议) 的代理。 </p><p>有了这类中间件的代理，自然而然就会想着做些什么，比如 抽象一些通用业务能力，在代理中进行一些特定处理……， 这样，也就和 dapr 殊途同归了……</p><h3 id="趋势"><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h3><p>可以看到一个趋势，随着技术需求的逐渐固化，开发模式逐渐靠近 <code>更加通用</code>、<code>更加透明</code>、<code>更加傻瓜式</code> 的设计理念，不论是各类开发框架提供的 <code>开箱即用 的开发套件</code>，还是类似于 grpc 提供的 <code>rpc client server 代码生成能力</code>， 亦或是各类脚手架工具提供的 <code>http 基础代码、orm 代码 一键生成</code> 能力，还是 基础设施层提供的 <code>流量代理side car</code> 流量治理能力，以及还未怎么普及的 <code>应用运行时</code>(eg: dapr)。</p><p>可以认为，基础设施所追求的一个目标是： <strong>极大地 降低业务人员的心智负担， 让业务人员 回归 业务开发！</strong> 。 </p><h3 id="我们可以做些什么？"><a href="#我们可以做些什么？" class="headerlink" title="我们可以做些什么？"></a>我们可以做些什么？</h3><p>为了实现这个目标，我们可以无所不用其极。例如：</p><ul><li>在一定程度上，选择一套完善的开发框架，能够完成业务开发的需要 (http server、log、api、orm)；</li><li>选择一套部署方案，能够快速准备环境，部署应用 (gitlab、k8s 等)；</li><li>选择一套通用基础设施，以及对应的管理平台 (mysql、redis、kafka 等)；</li><li>选择一系列通用工具，例如 功能测试、接口测试、压测、低代码平台、yapi、k8s 调试工具(devspace 等)、oauth平台、账户中心、rbac 等等；</li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>实际上，<code>dapr</code> 的设计思想，和 <code>baas</code> 有着密不可分的联系。当我们盘点一下 baas 所提供的能力，例如 文档存储、事件通知 ，就会发现，和 dapr 提供的 state 管理、pub/sub(binding) 相对应。</p><h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><p><a href="https://philcalcado.com/2017/08/03/pattern_service_mesh.html">Pattern: Service Mesh</a><br><a href="https://developer.aliyun.com/article/785943">Dapr 在阿里云原生的实践-阿里云开发者社区</a><br><a href="https://mosn.io/blog/posts/multi-protocol-deep-dive/">MOSN 多协议机制解析</a><br><a href="https://www.servicemesher.com/envoy/intro/what_is_envoy.html">Envoy 是什么? · Envoy proxy中文文档</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>service</tag>
      
      <tag>service mesh</tag>
      
      <tag>istio</tag>
      
      <tag>linkerd</tag>
      
      <tag>sidecar</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>几种后端通信方式的杂谈</title>
    <link href="/longblog/posts/22_3_25_some_kinds_of_communication_in_backends.html"/>
    <url>/longblog/posts/22_3_25_some_kinds_of_communication_in_backends.html</url>
    
    <content type="html"><![CDATA[<h2 id="计算机的几种通信方式"><a href="#计算机的几种通信方式" class="headerlink" title="计算机的几种通信方式"></a>计算机的几种通信方式</h2><p>对于计算机而言，通信 是一件永远不可能被忽视的事情。通信 基本可以分为 进程内通信 和 进程间通信。 进程内通信的方式大致有： <code>① 共享内存</code>  <code>② 变量传递</code>。进程间通信又需要分为 在同一操作系统上 和 在不同操作系统上，同一操作系统上的进程间通信主要有 <code>① 共享内存</code>  <code>② 共享存储</code> <code>③ 管道(命名)</code> <code>④ 信号量</code> <code>⑤ 消息队列</code> <code>⑥ socket</code> 通信。不同操作系统上，则只能使用 <code>socket</code> 通信。</p><p>实际上，从上面可以看出，除了 <code>信号量</code> 的通信方式比较独特外，其他的通信方式都是基于 <code>共享</code> 或 <code>传递</code> 的进行。只是 共享的方式 和 传递的方式 有所差别。</p><h3 id="进程内通信"><a href="#进程内通信" class="headerlink" title="进程内通信"></a>进程内通信</h3><p>在 进程内通信时，共享内存 是可以被特定编程语言直接使用的内存格式，因此效率非常高；在进程间的共享内存上，内存是最底层的共享方式，因此内存格式需要自己约定，也就意味着所有传递的值 需要经过一定的格式转换，也就是 序列化的过程。</p><p>在 进程内通信时，变量传递可以分为 值传递 和 引用传递 ，所有的操作都是编程语言做的，我们只管使用即可。但在 进程间的消息传递时，我们则需要自己处理 消息格式 的问题，也即是 序列化和反序列化 的过程。</p><p>上述的这些通信方式，实际上是很通用的模型，不仅在操作系统层被使用，在应用层，我们基于这些模型造了很多工具或软件系统。例如，共享内存 我们有 <code>redis</code> 、<code>memcache</code> 等，共享存储 我们有 <code>mysql</code>、<code>etcd</code>、<code>mongodb</code> 等等，管道 我们有 <code>redis</code> ，消息队列 我们有 <code>kafka</code>、<code>zeroMQ</code> 、<code>rabbitMQ</code> 等等，甚至有一些 消息队列的通用协议，例如 <code>MQTT</code>、<code>AMQP</code> 等等。</p><h3 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h3><p>最繁荣的进程间通信方式，实际上还是 <code>socket</code> 通信，socket 本身只是一个接口定义，不过这个接口定义太通用了，任何消息传递几乎都可以套用这个接口，无论是 <code>文件</code>、<code>网络</code> 还是 <code>内存</code> 。</p><h2 id="服务端的常见通信方式"><a href="#服务端的常见通信方式" class="headerlink" title="服务端的常见通信方式"></a>服务端的常见通信方式</h2><p>对于服务端开发而言，最常用的还是基于 网络 的 socket 编程，最底层自然都依赖 TCP/UDP，在应用层上，我们有非常多的协议，例如 <code>http</code> 、<code>http2</code> 、 <code>http3</code> 、<code>websocket</code>、 <code>MQTT</code> 以及各种小领域的协议，其实，所有的协议，都是对特定领域的通信内容的通用抽象，例如，http 是一个很常用的互联网请求协议，包含 请求行 、请求头 、请求体 ，一般认为 http 是一个 文本协议，实际上这指的是其 序列化方式，文本内容直接通过 ascll [待验证] 进行传输，背景是 http 协议出生在 文本内容共享浏览 的互联网初始定位 的时代。</p><p>后来发现 http 的传输效率不高，于是 http2 做了 压缩消息头 、二进制分帧传输、连接复用 的优化。在后来，认为基于 tcp 的传输效率还是有点低，而现代基础设施整体是比较好的，因此基于 udp 在应用层做了一些消息质量保证的操作，用于提升传输效率。</p><p>其实 http 系列协议，都可以看做是在 http1.1 的协议约定基础上，在传输层做优化。 那么，是否有其他通过优化协议本身内容的方式呢？其实其他协议都是在做这件事。例如，<code>websocket</code>，建立在 tcp 之上，使用非常简洁的控制帧，通信消息全在消息体当中。还有一些追求极致性能的协议，甚至直接建立在 TCP 上做应用层消息传输，例如 kafka 的通信协议。</p><p>从 自定义消息结构 这个看待协议的角度出发，我们甚至可以认为 rpc 才是真正的一切协议的源头，可以想象曾经有各类自定义的 rpc 格式，例如 <code>samba</code>、<code>nfs</code>、<code>ssh</code> 等等，当他们具有一定的名气后，大家就把他们从 自定义 rpc 协议 的认识中拎出来，直接用他们的名字代替。也就是说，我们现在所自定义的各种 rpc 协议，当他们具有一定名气后，就可以拥有自己的 名字 。</p><p>阐述完各类协议的基本情况后，我们来聚焦一下 <code>http</code>、<code>http2</code>、<code>grpc</code>、 <code>ws</code> ，看一下他们的关系。</p><h3 id="几种协议的区别认识"><a href="#几种协议的区别认识" class="headerlink" title="几种协议的区别认识"></a>几种协议的区别认识</h3><p>Http 的全名叫 超文本传输协议，是互联网最常用的协议，分为 请求行 (method + path)、请求头 (header) 、请求体 (body)，请求体可以是二进制数据(经过编码传输)。</p><p>http2 是 http 的升级版，所有请求格式延续了 http 的格式，对上层应用来说整体没啥差别(如果仅当做http来用)，但是也提供了 服务端推送、stream 等功能。一定程度上，我们可以认为，http2 是 http 的 传输层封装 (充当 transport 层)。</p><p>grpc 是把 http2 当做传输层 (transport 层)。其他特性是自行实现的，例如 <code>interceptor</code>、<code>resolver</code>、<code>balancer</code>、<code>auth</code>、<code>log</code>、<code>status</code>(状态码)、<code>stats</code>(监控)。实际上，我们不能把 grpc 当做一种协议，而是一种 rpc 框架 (类似于 http 框架)，协议 有特定格式 或 接口约定，而 grpc 是用于生成特定 server 和 client 的一整套工具。</p><p>ws 是直接在 tcp 之上的一层通信协议，特点是 轻量、连接保持，ws的想象空间很大，实际上，如果你愿意，甚至可以使用 ws 作为 http 的 transport 层，也可以把 ws 作为 mqtt 的 transport 层，也可以把 ws 作为 grpc 的传输层。</p><p>一些情况下，我们可以认为，一个协议 或者 一个框架，为什么选择了某项技术 而并不是 其他技术，是由于 生态 决定的，例如为什么 grpc 选用了 http2 而不是直接的 tcp 连接？为什么后端服务调用大家使用 grpc 而不是 手写 http？为什么后端不用 ws 通信？</p><h3 id="ws的特点"><a href="#ws的特点" class="headerlink" title="ws的特点"></a>ws的特点</h3><p>ws 现在的主要场景在前后端的即时消息上，这得益于 ws 的 状态保持 的特性，那么，我们是否可以基于这个特性，做更多的事情？</p><p>比如，① 基于 ws 的 http 协议转换、② 基于 ws 的 grpc 协议转换、③ 基于 ws 的自定义 rpc 框架、④ 基于 ws 的自定义框架。</p><p>对于 ①，应用场景较少，如果是为了传输性能，那么使用 http2 就能解决，而且 http2 的生态更好。<br>对于 ②，可以，但目前已经有了基于 http 的 grpc 协议转换，使用 http2 的情况下，性能也没啥问题。<br>对于 ③，可以，但要考虑生态问题，这基本意味着重新实现整套 grpc 的各模块。<br>对于 ④，可以，但目前已经有一些 ws 框架，例如 socket.io，要考虑清楚为什么需要一套新的框架。</p><h3 id="我对于当前-http-、http2、grpc、ws-方式的基本判断"><a href="#我对于当前-http-、http2、grpc、ws-方式的基本判断" class="headerlink" title="我对于当前 http 、http2、grpc、ws 方式的基本判断"></a>我对于当前 http 、http2、grpc、ws 方式的基本判断</h3><ol><li>http 有完善的接口定义方式 (<code>openapi</code>)，http2 甚至 http3 在性能上和将来的生态上也非常不错。目前没有看到好的基于 http 接口定义 方式自动生产 server 和 client 的工具。</li><li>grpc 有完善的接口定义方式，性能上和生态上很不错，有自动生成代码的工具链。前端调用不支持直接通信，需要经过 http 协议再转一次(浏览器端 或 proxy 端)。</li><li>ws 有一定的生态支持，ws 的状态保持在一些场景下是非常不错的特性。ws 没有自动生成代码的工具。</li></ol><h3 id="ws有什么特殊的价值？"><a href="#ws有什么特殊的价值？" class="headerlink" title="ws有什么特殊的价值？"></a>ws有什么特殊的价值？</h3><p>ws 有两个特性： ① 连接保持  ② 协议轻量 。另外，ws 的生态不错 (主要指浏览器的特殊支持)。<br>ws 在一些需要长链接的场景下，非常有价值，比如： ① 协商缓存内容，② 服务端缓存内容(eg: 权限)</p><p>如果解决 ws 的 ① 重连状态保持  ② http 降级  ③ 代码自动生成  ④ 开发模式  ⑤ 测试工具包 问题，那么 ws 不失为一个很好的通信工具。</p><h2 id="代码生成的思考"><a href="#代码生成的思考" class="headerlink" title="代码生成的思考"></a>代码生成的思考</h2><p>代码生成是一个非常好的思路，可以保证代码的统一性，减少不规范的地方，可维护性更高。现在可以看到，在 client 和 server 代码生成上，grpc 做的是最好的，生态也比较开放，在这个基础上可以开发一些自己需要的功能。</p><p>Go-zero 框架是自己实现的一套语法解析并形成特定代码，提供了模板化的方法生成代码，也和 grpc 一样提供了自定义插件的方式，看上去野心不小。</p><p>Go-kratos 则是接入 grpc 的生态，通过扩展生成代码的方式，接入了自己的 http 和 rpc。</p><p>Go-frame 在接口自动化代码生产上没有动作，只是在 脚手架工具 中简化了 对 grpc 代码生成的命令。</p><p>除了上面说到的 基于接口文档 自动化生成 server 及 client 代码外，还有一些其他常用的可生成的代码：</p><ol><li>基于 数据库表 生成 结构体、orm、基本 crud 代码。</li><li>基于 接口定义，生成前端 client 代码。</li><li>生成部署侧的脚本或包 (docker、k8s、devspace等)</li></ol><p>第 1 点中的 model 生成，go-frame 和 go-zero 都有做。 第 2 点目前只有 go-zero 做了一些，grpc 也有一些。 第 3 点都有动作。</p><p>另外 ，补充一嘴，数据库的结构体生成 是可以 正反使用的，例如，通过 orm，生成数据库表，同样也可以通过 数据库表，生成 orm，这点可以参考 <a href="https://blog.longalong.cn/posts/22_03_21_%E5%85%B3%E4%BA%8E%E5%9F%BA%E4%BA%8E%E6%8E%A5%E5%8F%A3%E5%AE%9A%E4%B9%89%E7%9A%84%E5%BC%80%E5%8F%91%E6%B5%81.html">关于基于接口定义的开发流</a> 最后的链接。</p><h3 id="一个疑惑"><a href="#一个疑惑" class="headerlink" title="一个疑惑"></a>一个疑惑</h3><p>按理，http 才是互联网下的王者，那么，为什么很少见到基于 http 的 接口定义文档 自动生成代码的工具呢？</p><p>实际上，相比于 protobuff 的 proto3 这种新的 DSL ，我们使用已有语言的成本可能更低，例如 基于 yaml 或者 基于 json 的，比如使用 openapi 的接口定义。甚至，使用一门我们熟悉的语言做接口描述，例如 js，然后代码生成则直接使用 js 进行拼接 ( 或模板渲染 )。</p><p>这个可以找找是否有相关的工具，如果没有，可以自己实现一个。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>杂七杂八写了一些自己的想法，很多表述不一定很精准，但思路确实还是有可参考性的，可以经常回味一下。</p>]]></content>
    
    
    
    <tags>
      
      <tag>communication</tag>
      
      <tag>backend</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>记一次有状态服务的负载均衡方案探索</title>
    <link href="/longblog/posts/22_3_23_a_record_of_balancing_stateful_service_explore.html"/>
    <url>/longblog/posts/22_3_23_a_record_of_balancing_stateful_service_explore.html</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>我们的服务是有状态服务，服务在提供对外访问时，需要负载到特定节点。</p><h2 id="基本方案"><a href="#基本方案" class="headerlink" title="基本方案"></a>基本方案</h2><p>一般来说，这种特定节点的负载均衡有两种基本方案：</p><ol><li>重定向</li><li>代理模式</li></ol><p>一起来看一下几个有状态服务的集群处理方式：</p><ul><li>Redis 在 6.0 以前是直接的采用 重定向 的方式，6.0 也提供了集群内代理的模式。 </li><li>Redis 的 codis 版集群，采用的是 代理模式。</li><li>Mycat 数据库代理采用的是 代理模式。</li><li>Mongodb cluster 采用的是 mongos 代理。</li><li>Kafka 采用的是客户端自定义的负载均衡方式，整体来看是 重定向方式 (根据 partition 的位置决定地址)。</li></ul><p>基本可以认为，负载均衡的这两种策略没有太大的优劣之分，只要实现好 client，对业务方来说，区别不大。</p><p>由于代理模式对机器资源的消耗更多，并且将来维护更加复杂，于是我选择先采用 重定向 的策略，这需要有两方面改动：</p><ol><li>所有节点均知道特定的 key 应该到哪一个具体的 节点。</li><li>返回的重定向数据，能够达到正确的节点。</li></ol><p>下面分别解决这两个问题。</p><h2 id="解决负载信息同步问题"><a href="#解决负载信息同步问题" class="headerlink" title="解决负载信息同步问题"></a>解决负载信息同步问题</h2><p>在业务侧，需要通过类似于 注册中心的机制，用于确定不同的 key 对应的 节点地址。这个注册中心有两种可供参考的模式： ① 无状态的包，所有状态通过中央存储( eg: redis/etcd ) 进行共享。② 状态交由特定的服务进行维护，其他 client 通过调用这个服务的接口获取信息。</p><p>第一种方案，类似于 k8s 的设计，所有源信息全在 etcd 中，各模块均通过监听 etcd 中的元信息变化做出自己的动作，这样做的好处是 轻量化，仅需要约定好数据结构即可，不用维护单独的服务，但为避免误用，需要提供 SDK。</p><p>第二种方案，类似于 mongodb 中的 config-server ，所有元信息交由 config-server 维护，其他节点 (mongos)通过本地缓存的方式提升性能。 这种方式的好处在于 权责分明，在没有太多精力维护 sdk 的情况下，这种方式更不容易出错。</p><p>其实，也可以认为还有第三种方案： 去中心化方案。类似于 redis 的集群通信方式，每个节点都存着一份整个集群的信息，并且通过一定的方式保证集群内数据一致性。但这种方式的实现更加复杂，也没有看到有什么更大的价值，暂不考虑。</p><p>在我的基本实现中，采用 抢占式 的模式，用 redis 做状态同步，整个流程类似于 “分布式锁” 的过程，可以达到负载到特定节点的目的，但整体比较粗糙，将来的可扩展性也不是很好。</p><p>不过值得参考的是，该实现中，采用了 redis 的 watch 机制，可以在各节点做本地缓存，有更新后也能更新缓存。这是一个很不错的技术点。</p><p>在将来要实现的版本中，应当是由一个服务来做负载均衡的策略，包括收集节点状态、新节点启动、老节点清理、数据迁移 等操作。 这部分可以更多参考 mongodb 的 config-server 相关设计。</p><p>在保证了注册中心机制后，就是网络路由问题了。</p><h2 id="解决-nginx-定向路由问题"><a href="#解决-nginx-定向路由问题" class="headerlink" title="解决 nginx 定向路由问题"></a>解决 nginx 定向路由问题</h2><p>由于服务是在内网中，也不能将内网服务的 ip 直接暴露在公网上，因此，要有从公网路由到内网特定节点的能力。</p><p>我们目前采用的是 k8s 的部署方式，网关处使用 nginx-ingress 进行路由 和 负载均衡。nginx-ingress 默认提供了 轮询、加权、hash、一致性hash 的负载均衡策略，且 hash 函数不是我们能指定的。因此，这些策略无法满足我们的需求。</p><p>不过 nginx-ingress 提供了自定义负载均衡策略的方式(通过 lua 脚本)，也就意味着我们能够自定义负载均衡策略。</p><p>以下是基本实现：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs lua"><span class="hljs-comment">-- file longbalancer.lua</span><br><span class="hljs-keyword">local</span> util = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;util&quot;</span>)<br><br><span class="hljs-keyword">local</span> string_format = <span class="hljs-built_in">string</span>.<span class="hljs-built_in">format</span><br><span class="hljs-keyword">local</span> ngx_log = ngx.<span class="hljs-built_in">log</span><br><span class="hljs-keyword">local</span> INFO = ngx.INFO<br><span class="hljs-keyword">local</span> _M = &#123;&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">_M.new</span><span class="hljs-params">(self, backend)</span></span><br>  <span class="hljs-keyword">local</span> o = &#123;<br>    name = <span class="hljs-string">&quot;longbalance&quot;</span><br>  &#125;<br>  o.addrs, o.addrList, o.nums = util.get_addrs(backend.endpoints)<br>  o.eps = util.get_nodes(backend.endpoints)<br>  o.nowLen = <span class="hljs-number">0</span><br><br>  <span class="hljs-built_in">setmetatable</span>(o, <span class="hljs-built_in">self</span>)<br><br>  <span class="hljs-built_in">self</span>.<span class="hljs-built_in">__index</span> = <span class="hljs-built_in">self</span><br>  <span class="hljs-keyword">return</span> o<br><span class="hljs-keyword">end</span><br><br><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">_M.sync</span><span class="hljs-params">(self, backend)</span></span><br><br>  <span class="hljs-keyword">local</span> eps = util.get_nodes(backend.endpoints)<br>  <span class="hljs-keyword">local</span> changed = <span class="hljs-keyword">not</span> util.deep_compare(<span class="hljs-built_in">self</span>.eps, eps)<br>  <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> changed <span class="hljs-keyword">then</span><br>    <span class="hljs-keyword">return</span><br>  <span class="hljs-keyword">end</span><br><br>  ngx_log(INFO, string_format(<span class="hljs-string">&quot;nodes have changed for backend %s&quot;</span>, backend.name))<br><br>  <span class="hljs-built_in">self</span>.addrs, <span class="hljs-built_in">self</span>.addrList, <span class="hljs-built_in">self</span>.nums= util.get_addrs(backend.endpoints)<br>  <span class="hljs-built_in">self</span>.eps = eps<br>  <span class="hljs-built_in">self</span>.nowLen = <span class="hljs-number">0</span><br><br><span class="hljs-keyword">end</span><br><br><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">_M.balance</span><span class="hljs-params">(self)</span></span><br>  <span class="hljs-keyword">local</span> balance_by = ngx.var[<span class="hljs-string">&quot;balance_by&quot;</span>]<br>  <span class="hljs-keyword">if</span> balance_by == <span class="hljs-literal">nil</span> <span class="hljs-keyword">then</span><br>    balance_by = <span class="hljs-string">&quot;$docdoc&quot;</span><br>  <span class="hljs-keyword">end</span><br><br>  <span class="hljs-keyword">local</span> balance_val = util.lua_ngx_var(balance_by)<br><br>  ngx_log(INFO, string_format(<span class="hljs-string">&quot;balance key is : %s, val is : %s&quot;</span>, balance_by,balance_val))<br><br>  <span class="hljs-keyword">return</span> <span class="hljs-built_in">self</span>.<span class="hljs-built_in">find</span>(<span class="hljs-built_in">self</span>, balance_val)<br><span class="hljs-keyword">end</span><br><br><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">_M.getnext</span><span class="hljs-params">(self)</span></span> <br>    <span class="hljs-keyword">local</span> addr = <span class="hljs-built_in">self</span>.addrList[<span class="hljs-built_in">self</span>.nowLen]<br><br>    <span class="hljs-built_in">self</span>.nowLen = <span class="hljs-built_in">self</span>.nowLen + <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">self</span>.nowLen == <span class="hljs-built_in">self</span>.nums <span class="hljs-keyword">then</span><br>      <span class="hljs-built_in">self</span>.nowLen = <span class="hljs-number">0</span>    <br>    <span class="hljs-keyword">end</span><br><br>    <span class="hljs-keyword">return</span> addr<br><span class="hljs-keyword">end</span><br><br><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">_M.find</span><span class="hljs-params">(self, balance_val)</span></span><br>  <span class="hljs-keyword">local</span> addr<br><br>  <span class="hljs-keyword">if</span> balance_val == <span class="hljs-literal">nil</span> <span class="hljs-keyword">or</span> balance_val == <span class="hljs-string">&quot;&quot;</span> <span class="hljs-keyword">or</span> balance_val == <span class="hljs-number">0</span> <span class="hljs-keyword">then</span><br>    addr = <span class="hljs-built_in">self</span>.getnext(<span class="hljs-built_in">self</span>)  <br>    <span class="hljs-keyword">return</span> addr<br>  <span class="hljs-keyword">end</span><br><br>  addr = <span class="hljs-built_in">self</span>.addrs[balance_val]<br>  <span class="hljs-keyword">if</span> addr == <span class="hljs-literal">nil</span> <span class="hljs-keyword">then</span><br>    addr = <span class="hljs-built_in">self</span>.getnext(<span class="hljs-built_in">self</span>)  <br>  <span class="hljs-keyword">end</span><br><br>  <span class="hljs-keyword">return</span> addr<br><span class="hljs-keyword">end</span><br><br><span class="hljs-keyword">return</span> _M<br><br><span class="hljs-comment">-- file util.lua</span><br><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">_M.get_addrs</span><span class="hljs-params">(endpoints)</span></span><br>  <span class="hljs-keyword">local</span> addrs = &#123;&#125;<br>  <span class="hljs-keyword">local</span> addrList = &#123;&#125;<br>  <span class="hljs-keyword">local</span> nums = <span class="hljs-number">0</span><br><br>  <span class="hljs-keyword">for</span> _, endpoint <span class="hljs-keyword">in</span> <span class="hljs-built_in">pairs</span>(endpoints) <span class="hljs-keyword">do</span><br>    addrs[endpoint.address] = endpoint.address .. <span class="hljs-string">&quot;:&quot;</span> .. endpoint.port<br>    addrList[nums] = endpoint.address .. <span class="hljs-string">&quot;:&quot;</span> .. endpoint.port<br>    nums = nums + <span class="hljs-number">1</span><br>  <span class="hljs-keyword">end</span><br><br>  <span class="hljs-keyword">return</span> addrs, addrList, nums<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><p>然后在 balancer.lua 文件中导入 longbalancer 即可。</p><p>另外，为了服务能够使用正确的负载均衡策略，需要在 服务的 ingress 中添加如下注解</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">nginx.ingress.kubernetes.io/configuration-snippet: |<br>      set $docdoc $arg_insip; # 设置负载均衡参数<br>nginx.ingress.kubernetes.io/load-balance: longbalance  # 选择负载均衡策略<br></code></pre></td></tr></table></figure><p>自此，nginx 拥有了根据特定的参数进行定向路由的能力。<br>[鼓掌 ！ 👏]</p><p>这里实际上是有优化空间的，有两个方向：</p><ol><li>添加 缓存 =&gt; documentID : insip 在nginx进行缓存，没有传 insip 的参数时，先通过缓存判断，没有再走轮询。</li><li>直接接入向 config-server 访问的能力，在网关层直接定位到确定的节点，而不是靠重定向。</li></ol><p>各有优劣，之后再做分析</p>]]></content>
    
    
    
    <tags>
      
      <tag>load balancing</tag>
      
      <tag>stateful</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>grpc在k8s中的负载均衡问题</title>
    <link href="/longblog/posts/22_3_23_grpc_balancing_problem_in_k8s.html"/>
    <url>/longblog/posts/22_3_23_grpc_balancing_problem_in_k8s.html</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>因为 grpc 是基于 http2 的通信，而 http2 对单个 endpoint 默认仅建立一条 TCP 连接，这就导致在 k8s 中，一个 service 默认仅会有一条 grpc 连接，并且，对于该 grpc 的请求，也都会集中到其中一个 pod 上。</p><p>尽管 k8s 的 service 本身有着 round robin 的负载均衡方式，但那都是建立在 “多次建立连接” 的基础上，对于已经建立连接后，基于 四层网络通信 的 TCP，是无法做到负载均衡的。<br>这个问题在我们当前的服务中也存在，两个服务间通过 service name 进行调用时，则会出现负载不均问题。</p><p>之前一直没有太重视这个问题，主要原因在于 每个服务都会有多个 pod ，那么多个 pod 调用 多个 pod 时，一定程度上进行了负载均衡。但是这种负载均衡很不稳定，比较容易出现连接集中到其中几个 pod 上的情况，因此，需要用其他方式解决。</p><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>经过阅读 grpc 代码，发现 grpc 本身有提供一定的机制解决负载均衡的问题，只是默认的方式在 k8s 中没有那么友好。<br>这其中涉及到 两个主要概念： resolver、balancer(picker)</p><p>resolver的作用，是解析一个服务地址对应多少 ip 地址，默认的方式是 passthrough，意味着透传服务地址，交由更底层的 transport 去处理。</p><p>还有一些其他的 resolver： ① DNS resolver  ② Manual  resolver ③ unix resolver</p><p>其中，DNS resolver 可以解析 DNS 中所挂载的 backend ips，这对于传统的基于 DNS 做负载均衡的方案比较好用，k8s 中的 statefulset 也可以大致基于这种模式。</p><p>Manual resolver 则是手动设置 backend ips，如果有自己的服务注册与服务发现机制，则用这种方式就比较方便。</p><p>Balancer 的作用，是从 resolver 解析的对应的 ip 地址池 选择特定的连接，其中核心的职能由 picker 承担，grpc 提供了大量的负载均衡策略，并且支持自定义策略，默认是 pick_first，还有一些例如：轮询、加权轮询、grpc远程lb、优先、rls(自适应？)。 甚至，grpc 提供了一些集群负载均衡的策略，例如一致性hash、CDS LB？等。</p><p>从上面分析来看，我们至少有这么两类解决方案：</p><ol><li>通过使用 grpc 本身提供的 resolver 机制 和 balancer 机制，实现基于 k8s 的服务发现机制(通过client-go 进行封装)，则能比较优雅地解决这个问题。</li><li>通过在 client 端实现 conn-pool 的方式，类似于通过多次 dial 的方式创建多个连接，然后自行实现一些 负载均衡的策略，例如 round-robin 或者随机，或者 sticky 的机制等。这个方案实现起来，从当前的技术复杂度上来看是最低的。但有三个问题： ① service 本身一定要更加“随机”，如果是 sticky 类机制，则此方式失效(k8s service默认是轮询机制)。 ② 每个遇到 grpc 负载均衡问题的 client ，都要改动其 client 包，以支持获取 conn 的方式(或者进行多一层封装，github.com/shimingyah/pool 就是采用的这种方式) 。③ 连接是在初始化过程建立的，初始化之后通过扩容形成的新pod很难被加入到连接池中。</li></ol><p>另外，服务网格也有一定的方式解决这个问题，这是代理工具做的优化，例如 envoy 和 nginx 都有针对 grpc 的优化，但这个方案太重(我们目前没有使用服务网格)，暂不考虑。</p><p>从目前来看，我认为第一种方式更优雅，对业务的侵入也更小，仅需要修改 grpc 的 dial options，以及导入一个包即可。 这个包的设计，最好将 服务发现 独立出来，专门用于 k8s 中的服务发现与动态监听。</p><p>值得一提的是，在 go-zero 以及 go-krotas 中，均有解决这个问题的方案，均是通过上述添加 k8s resolver 的方式解决的。如果要自行解决，可以参考。</p><h2 id="坑点"><a href="#坑点" class="headerlink" title="坑点"></a>坑点</h2><ol><li>如果要采用 k8s 的 list endpoints 和 watch endpoints 机制，则需要添加特定的 role，绑定相关的权限，否则会 panic。</li><li>在使用 k8s 的 sdk 时，要注意版本对应，否则可能会遇到奇怪的问题，甚至会有 golang 编译版本的问题 (最新甚至要求 1.17)。</li></ol><p>另外可以参考：<br><a href="https://zhuanlan.zhihu.com/p/258326212">Kubernetes中gRPC Load Balancing分析和解决</a><br><a href="https://segmentfault.com/a/1190000004492447">负载均衡算法及手段</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>grpc</tag>
      
      <tag>load balancing</tag>
      
      <tag>k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于基于接口定义的开发流</title>
    <link href="/longblog/posts/22_03_21_develop_flow_based_on_interface_definition.html"/>
    <url>/longblog/posts/22_03_21_develop_flow_based_on_interface_definition.html</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>之前做前端开发，需要依赖后端同学的接口文档，涉及到一些老接口的时候，经常出现接口信息疏于维护的情况，然后找这个找那个的，依然很难做到让接口信息完整，当时就在想，为什么不再要修改一个接口的时候，把接口文档顺便维护了呢？心中甚是鄙夷。</p><p>后来做后端开发，经常出现一些要求快速完成进行联调的情况，更甚至有临时改变一些功能的情况，自己可能顺手就加上了，然后上到测试环境让前端同学调一下，然后……就没有然后了，因为运行起来没什么问题，也没有谁让把接口文档补全，就这样搁置了。</p><p>再后来后端开发也要兼顾接口测试，接口测试 的用例梳理，是基于接口定义，一方面找到接口的参数边界，另一方面测试完整的场景，这时候才发现，之前的接口文档写得是多么简陋。<br>于是，就开始探索更好的后端接口开发工作流。</p><h2 id="灵感来源及畅想"><a href="#灵感来源及畅想" class="headerlink" title="灵感来源及畅想"></a>灵感来源及畅想</h2><h3 id="灵感1"><a href="#灵感1" class="headerlink" title="灵感1"></a>灵感1</h3><p>接口文档的维护大体上分为两派，① 接口文档应当跟随代码，例如 swagger  ② 接口文档管理在专门的接口平台，例如 yapi。 这两派都有其拥护者。目前看到的，市场上很多企业还是走的第 ② 条路，主要原因，可能还是对后端人员来说上手简单，有好用的 UI，写接口就点点点几下；再加上给前端同学交付比较方便(仍一个链接过去)，带有 mock 功能，可能还有一些简单的 接口测试 功能，用起来很流畅。 用这种方式的一个弊端，可能就是 <code>容易疏于维护</code>。</p><p>基于代码的方案基本就是 swagger 的方式，在定义接口时，用 <code>注释</code> 或者 <code>装饰器</code> 等方式，将接口的元信息嵌入到代码中，这样就可以做到修改代码的时候，就可以顺便修改接口信息。 但这样的弊端就是： ① 部署不方便，yapi 可以独立部署，很稳定，基于代码的则可能随意改变，对使用方而言不够稳定。 ② mock 、test 等功能较弱，有些时候需要提供 这些功能时比较容易被要求更换。</p><h3 id="灵感2"><a href="#灵感2" class="headerlink" title="灵感2"></a>灵感2</h3><p>之前系统还不是很复杂时，主要对外提供 http 接口，后来，服务越来越多，服务间的通信则主要使用 rpc 方式，我们选择的 grpc 框架，该框架在创建接口时的方式，是基于 proto3 定义的接口，使用 protoc 直接生成对应的代码，将接口定义变成真正的 <code>接口</code>，然后由开发者自行实现该接口，达到提供真正的接口功能的目的。</p><p>我在想，http 和 rpc，都是远程通信的方式，从目的上看，本就没什么区别。看起来，只是 rpc 是基于自定义的 <code>数据格式</code> 及 <code>序列化方式</code>，而 http 整体看是个<code>通用的文本协议</code>。那么，是不是可以把 grpc 中的一些优点，应用到 http 的请求中来，甚至应用到 ws 等长链接协议中？</p><p>后来，看到 go-zero 这个项目中，脚手架工具的 go-ctl 就已经包含了这项功能，基于一个自定义的接口定义文档，生成一定模板下的基本代码。觉得十分好用。</p><h3 id="更多探索"><a href="#更多探索" class="headerlink" title="更多探索"></a>更多探索</h3><p>在探索的过程中，观摩了几个 web 开发框架，可以算得上是典型了：</p><ul><li> <a href="https://goframe.org/pages/viewpage.action?pageId=1114399">goframe</a></li><li> <a href="https://go-zero.dev/cn/">go-zero</a></li><li> <a href="https://go-kratos.dev/docs/">go-kratos</a></li></ul><p>他们分别有自己的开发工具体系。</p><p><code>goframe</code> 是采用 <code>将接口信息写在代码中</code> 的方式，通过解析 request 和 response 中的 meta，以及在注册路由时添加的一些额外信息，生成 openapi 格式的 json 文档，实际上，这也是生成 swagger 所需要的信息，然后直接在框架中继承了 swagger。 对于开发者而言，仅需要关注代码即可，在代码中通过 <code>tag</code> 或者 显式描述 的方式，即可保证接口文档与接口实现一致，改动成本较小，学习成本较低。</p><p><code>go-zero</code> 是自己实现了一套 接口定义 的语法，整体类似于 golang 和 proto3 的结合体，词法语法的解析均是自己实现的，这也就意味着，go-zero 可以完全掌控 接口定义文档 到代码的生成过程，因此，其脚手架工具 <code>go-ctl</code> 提供了 api 和 rpc 两种接口代码自动生成的方式，开发者仅需要写 接口定义文档 即可生成基本代码，然后自己补充业务逻辑即可。另外，生成的基本代码是根据 模板 确定的，这也就在一定程度上支持了自定义生成代码的功能，值得更多探索。另外，生态中也有将 接口定义 和 swagger 格式对接 的工具了，可以生成 swagger 文档。</p><p><code>go-kratos</code> 采用的是 grpc 的一套方案，grpc 的 语法解析 是 C++ 实现的，然后生成一套自定义 ast 的 <code>FileDescriptor</code> 结构体，这个结构体通过编码成二进制传递给其他程序；grpc 的代码生成是由各类 plugin 实现的，例如我们常用的 golang 的代码，则是由 <code>protoc-gen-go</code> 这个插件生成的(实际上有好几个常见的实现)。那么，也就意味着，只需要再实现一个将 proto3 的接口定义 转成一套 http 的代码的 plugin，也就可以实现和 grpc 一样的代码生成能力。 这也就是 go-kratos 采用的方案，插件的名字叫 <code>protoc-gen-go-http</code> 。 和 go-zero 类似，go-kratos 也提供了生成 swagger 的<a href="https://github.com/go-kratos/swagger-api">工具</a>。</p><p>基于接口定义，我们可以做很多事情，例如： ① 基本代码自动生成(server、client) ② 接口文档自动生成 ③ 基本接口测试代码自动生成 ④ 协议转换 ⑤ mock 。有了这些自动化手段，做开发时就可以很纯粹地关心业务逻辑即可，这对于降低开发的复杂度而言，有着巨大的价值。</p><h2 id="如何行动起来"><a href="#如何行动起来" class="headerlink" title="如何行动起来"></a>如何行动起来</h2><p>探索这些的原因，在于我们当前的 ws 通信，接口层是自定义的 <code>消息码</code>，前后端都需要维护一套必须一样的枚举值，并且前后端都要对每个消息码写一套大致相同的代码，十分繁琐。于是就想到，如果采用了和 grpc 一样的自动生成 服务端 和 客户端 代码的方式，那么 接口管理复杂度、代码复杂度、代码规范、健壮性、可调试性 等各方面都将得到很大的提升。</p><p>一切想法，要想产生真实的价值，就必须得要落地，而落地的方法，可以以实践为目标进行梳理，当有一定思路时，就赶紧行动起来。</p><p>目前，我可以先： ① 阅读 protobuff 的代码生产方式(c++写的，几乎完全没读懂)  ② 阅读 go-ctl 的代码生成方式 (自实现的 ast 解析，代码比较规范，读懂大概)  ③ 阅读 goframe 的接口文档生成方式 (自定义的结构以及转换，比较易懂)</p><p>然后： ④ 分析不同类型接口的差异 ⑤ 梳理 ws 生成的代码应该有些什么部分  ⑥ 以 ws 为例，开发一版基本代码</p><p>另外，补充一些其他维度的TODO：</p><ol><li>重新学习一遍编译原理，弄清楚 词法解析 和 语法解析，以及将自定义语法变成 elf 格式的过程。</li><li>学习使用 正则 和 有限状态机 的方式进行词法、语法解析。</li><li>对 go-frame、go-zero、go-kratos、gin、go-kit 这几个框架进行更深层次的对比，从他们提供的功能 以及 功能的实现方式 等方面进行对比。</li><li>从开发效率的角度，探索各类工具对效率的提升程度，包括维护成本在内。可以先看看：<ul><li> <a href="https://github.com/flipped-aurora/gin-vue-admin">gin-vue-admin</a></li><li> <a href="https://juejin.cn/post/7034813841833721893">6 个 golang 在线工具</a></li></ul></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>grpc</tag>
      
      <tag>develop</tag>
      
      <tag>openapi</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>记一次k3s环境搭建记录</title>
    <link href="/longblog/posts/21_12_26_a_record_of_k3s_run_up.html"/>
    <url>/longblog/posts/21_12_26_a_record_of_k3s_run_up.html</url>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>生产环境中，我们很早在使用 k8s 作为基础设施了，后来，企业项目要做私有化部署，就把目光转向 <code>k3s</code> 了。<br>k3s 是一个轻量级的 k8s，几乎实现了所有 k8s 的特性，除了一些边缘的场景，基本可以把 k3s 当做 k8s 去使用。</p><h3 id="一些备选方案"><a href="#一些备选方案" class="headerlink" title="一些备选方案"></a>一些备选方案</h3><p>早期部署 k8s 环境是十分繁杂的，各个组件都是二进制方式部署，如果遇到问题，排查起来需要的预备知识非常多，因此，大部分企业在使用 k8s 时，都是直接选择云服务商提供的 k8s 集群，阿里云、腾讯云、亚马逊等等都提供了非常完善的 k8s 集群。<br>那作为个人想要玩一玩 k8s ，我们可以怎么搞呢？ 其实社区已经出了非常多的用于简化 k8s 环境搭建的项目，下面列举一些：</p><ol><li>使用 ansible 等工具简化搭建，例如 <a href="https://github.com/kubernetes-sigs/kubespray">kubespray</a>、<a href="https://github.com/easzlab/kubeasz">kubeasz</a>、<a href="https://github.com/kubernetes/kops">Kops</a>、<a href="https://github.com/kubernetes/kubeadm">kubeadm</a></li><li><a href="https://github.com/kubernetes/minikube">minikube</a>、<a href="https://github.com/kubernetes-sigs/kind">kind</a>、<a href="https://github.com/ubuntu/microk8s">microk8s</a>、<a href="https://github.com/k3s-io/k3s">k3s</a> 等轻量级 k8s 实现。</li><li><a href="https://github.com/ubuntu/microk8s">rancher</a>、<a href="https://github.com/KubeOperator/KubeOperator">kubeoperator</a> 这类可视化操作方式</li><li>如果你只是想在自己电脑上玩一下，可以直接使用 docker 客户端提供的 k8s 集群，或者用 <a href="https://github.com/rancher/k3d">k3d</a> (k3s in docker) 、<a href="https://github.com/kubernetes/minikube">minikube</a>、<a href="https://github.com/kubernetes-sigs/kind">kind</a></li></ol><p>值得一提的是， k3s 和 microk8s 是为生成环境设计的 k8s 实现，是为了在资源有限的情况下使用 k8s 集群的一种方案，例如 边缘计算、iot 等场景。(可以搜索和 kubeedge 等结合的内容)</p><h3 id="搭建操作"><a href="#搭建操作" class="headerlink" title="搭建操作"></a>搭建操作</h3><p>因为更加看好 k3s 的生态，因此选用 k3s 作为基础设施。在 k3s 的搭建上，使用官方的搭建脚本已经比较简单了，不过还是有一些为了更加简化搭建过程而出现的项目，例如 <a href="https://github.com/alexellis/k3sup">k3sup</a> 、 <a href="https://github.com/cnrancher/autok3s">autok3s</a> 这两者都提供了远程安装 k3s 的能力，不过后者还有图形化界面。 从搭建难度上，这两者对我而言没啥差别，因此我直接选择了 k3sup。</p><ol><li>在本地下载 k3sup </li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl -sLS https://get.k3sup.dev | sh<br></code></pre></td></tr></table></figure><ol start="2"><li><p>在阿里云上开两台机器，并配置公钥</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-copy-id root@xxx.xx.xx.xx<br></code></pre></td></tr></table></figure></li><li><p>在机器 1 上部署 master 节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">k3sup install --user root --ip xx.xx.xx.xx --k3s-version v1.21.1+k3s1  --k3s-extra-args &#x27;--no-deploy traefik --docker&#x27; --tls-san &quot;xx.xx.xx.xx&quot; --context k3s --merge<br></code></pre></td></tr></table></figure></li><li><p>在机器 2 上部署 slave 节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">k3sup join --user root --ip xx.xx.xx.xx --k3s-version v1.19.7+k3s1 --server-ip xx.xx.xx.xx<br></code></pre></td></tr></table></figure></li><li><p>查看结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longk3s001 ~]# kubectl  get node<br>NAME         STATUS   ROLES    AGE    VERSION<br>longk3s001   Ready    master   129m   v1.19.7+k3s1<br>longk3s002   Ready    &lt;none&gt;   112m   v1.19.7+k3s1<br></code></pre></td></tr></table></figure></li></ol><p>以上，基本的 k3s 环境就搭建完成成功了</p><p>可以设置几个常用的 kubectl 别名，方便使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &lt;&lt;eof &gt;&gt; ~/.bashrc<br>alias k=kubectl<br>alias ke=&quot;kubectl edit&quot;<br>alias ked=&quot;kubectl edit deploy&quot;<br>alias kg=&quot;kubectl get&quot;<br>alias kgp=&quot;kubectl get po&quot;<br>alias wp=&quot;watch kubectl get po&quot;<br>alias kd=&quot;kubectl delete&quot;<br>alias kdp=&quot;kubectl delete pod&quot;<br>alias klf=&quot;kubectl logs -f&quot;<br>alias krrd=&quot;kubectl rollout restart deploy &quot;<br>eof<br>bash<br><br></code></pre></td></tr></table></figure><h3 id="解决远程访问问题"><a href="#解决远程访问问题" class="headerlink" title="解决远程访问问题"></a>解决远程访问问题</h3><p>在集群外访问集群内的通用方案，基本可以分为： </p><ol><li>loadBalancer</li><li>nodePort</li><li>ingress</li><li>hostNetwork</li></ol><p>一般来说，云厂商提供的 k8s 集群，都是采用 loadBalancer 的方式，云厂商会自动提供公网 ip (当然也可以是内网 ip)。 而对于自建的 k8s 集群，则没有现成的 lb，要么使用其他方式，要么，自建 lb。</p><ol><li>自建 lb 可以采用 <a href="https://github.com/metallb/metallb">metalLB</a>， 另外，k3s 官方也提供了一个 lb 实现 <a href="https://github.com/k3s-io/klipper-lb">klipper-lb</a></li><li>使用 nodePort 是一个比较简便的方式，直接改 service 类型即可。但问题是要记各个服务的 nodePort 是多少，比较麻烦，服务多了之后，根本不记得哪个 port 对应哪个服务。</li><li>使用 ingress 是一个很不错的方式，相当于在所有服务前加了一个反向代理服务器，比如 nginx。ingress 的使用能比较好地解决端口复用问题，可以根据 二级域名、访问路径、header 等各种标识对流量进行分发，基本上可以认为，对于企业级项目，用 ingress 就对了。不过，ingress 的相关配置是一个需要学习的内容，尤其是关于 ssl 证书，之后专门出一篇文章记录一下。</li><li>使用 hostNetwork 相当于直接使用宿主机的网络，也就是说，一个服务若开了 8080 端口，则会直接在宿主机上监听 8080 端口。一般来说，hostNetwork 适用于一个集群仅服务于一个主服务的项目。但有一种很好的方式，可以解决 没有 lb 的问题，那就是 使用 ingress 作为流量分发，同时对 ingress 使用 hostNetwork，并且，对于 ingress 使用 daemonset 进行部署，这样就类似于将 ingress 作为 lb 来使用了。</li></ol><p>我直接采用 4 中的方案。相关的信息可以参考 <a href="https://github.com/kubernetes/ingress-nginx/blob/nginx-0.29.0/docs/deploy/baremetal.md">ingress-nginx</a></p><p>记录一下基本过程：</p><ol><li><p>进入到 nginx-ingress 的<a href="https://github.com/kubernetes/ingress-nginx/blob/nginx-0.29.0/docs/deploy/index.md">文档中</a></p></li><li><p>获取清单</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/mandatory.yaml<br></code></pre></td></tr></table></figure></li><li><p>修改 deployment 资源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 1. 修改 Deployment 为 DaemonSet</span><br><span class="hljs-meta">#</span><span class="bash"> 2. 去掉 spec.replicas</span><br><span class="hljs-meta">#</span><span class="bash"> 3. spec.template.spec 增加  hostNetwork: <span class="hljs-literal">true</span></span><br></code></pre></td></tr></table></figure></li><li><p>启用配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f mandatory.yaml<br></code></pre></td></tr></table></figure></li></ol><p>这之后就按照自己的需要，部署自己的服务即可。</p><h3 id="解决存储问题"><a href="#解决存储问题" class="headerlink" title="解决存储问题"></a>解决存储问题</h3><ul><li>使用默认的 本地存储 (local-path)。</li><li>使用 nfs 或其他远程存储方案，具体可以参见：<a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">nfs-provisioner</a></li><li>使用类似于 Longhorn 的分布式存储方案 (k3s 推荐方式)，具体可以参见：<a href="https://rancher.com/docs/k3s/latest/en/storage/">k3s storage</a></li></ul><h3 id="解决日志问题"><a href="#解决日志问题" class="headerlink" title="解决日志问题"></a>解决日志问题</h3><p>默认的日志是分布在各个节点上的，当节点被删除时，日志也就丢了，日志可以使用 fluentd 进行采集(或者 fluent-bit)，具体可以参见： <a href="https://docs.fluentd.org/">fluentd 官方文档</a></p><p>[TODO] 解决日志解析和查询问题 (重量和轻量)</p><h3 id="解决证书问题"><a href="#解决证书问题" class="headerlink" title="解决证书问题"></a>解决证书问题</h3><ol><li>解决集群内证书时长问题，虽然每年重启一次 k3s 即可自动更新证书，但一个稳定的服务扔在那里，谁记得啥时候要重启啊，可以通过改代码重新编译，变成100年就ok了。</li><li>解决 ssl 证书问题，可以参考 <a href="https://cert-manager.io/docs/">cert-manager</a></li></ol><h3 id="解决面板及管理工具"><a href="#解决面板及管理工具" class="headerlink" title="解决面板及管理工具"></a>解决面板及管理工具</h3><p>k8s 生态下已经有大量的面板，最基础的是官方的 dashboard，周边的还有：</p><ul><li>kubeboard</li><li>kubepi</li><li>kubesphere</li><li>rancher</li></ul><p>面板工具来看，整体差别不大，交互上有少量差别，日常使用完全够了，如果有多集群管理需求的话，个人使用不建议 kubeboard(超过3个收费) ，毕竟穷人不配(-.-!)。 kubesphere 和 kubeboard 的周边插件功能还是不错的，可以方便地集成一些常用的组件，并且提供了控制面板。</p><p>客户端还有 lens， 命令行还有 k9s ，都是非常不错的工具。尤其是 <code>k9s</code> ，熟悉了快捷键后，十分方便。</p><h3 id="解决-CD-问题"><a href="#解决-CD-问题" class="headerlink" title="解决 CD 问题"></a>解决 CD 问题</h3><p>对于 k8s 的自动发布，最基础的方式自然是在原有的 CD 脚本中，写一些 kubectl 的命令。但这样不够优雅，主要是将来维护比较麻烦，对于不够熟练 kubectl 的同学而言，有一定学习成本。</p><p>如果要采用更加成熟的方案，可以考虑 jenkins (x) ，社区也有很多针对 k8s 的脚本。 也可以采用 argoCD ，和 k8s 的生态结合得比较紧密。 droneCI 也是一个不错的选择。</p><p>还有一些备选方案： skaffold、devspace 等，这些是可以在本地打包，然后部署到 k8s 的方式。</p><p>如果，企业级使用的话，spinnaker 是一个非常不错选择。</p><p>[TODO] 补一些 CD 相关的文档</p><h3 id="解决监控问题"><a href="#解决监控问题" class="headerlink" title="解决监控问题"></a>解决监控问题</h3><p>机器的监控，比较轻量的方式可以使用 netdata。另外可以用 datadog，生态应该也是很不错的，不过我没怎么做更多的探索。<br>grafana + promethues 是一个更加通用的方案，社区也提供了大量的面板模板，不论是 node exporter 还是 pod exporter，都有比较成熟的面板。推荐使用。 </p><h3 id="解决开发调试问题"><a href="#解决开发调试问题" class="headerlink" title="解决开发调试问题"></a>解决开发调试问题</h3><p>两个不错的方案：<br><a href="https://devspace.sh/cli/docs/introduction">devspace</a><br><a href="https://nocalhost.dev/docs/quick-start/">nocalhost</a></p><h3 id="其他可能的问题"><a href="#其他可能的问题" class="headerlink" title="其他可能的问题"></a>其他可能的问题</h3><ul><li>服务可视化 (tracing、metrics)</li><li>日志体系</li><li>告警体系</li><li>均衡问题 可以参考 <a href="https://github.com/kubernetes-sigs/descheduler">descheduler</a></li></ul><p>TODO:</p><ol><li>增加 k8s ssl相关配置操作记录</li><li>增加 k8s 日志、监控、追踪、告警 相关操作记录</li><li>增加对一个服务的相关操作</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>develop</tag>
      
      <tag>k3s</tag>
      
      <tag>enviroment</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何解决linux下的代理访问</title>
    <link href="/longblog/posts/21_12_12_how_to_resolve_proxy_in_linux.html"/>
    <url>/longblog/posts/21_12_12_how_to_resolve_proxy_in_linux.html</url>
    
    <content type="html"><![CDATA[<h3 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h3><p>想在 linux 上安装 helm，结果网速巨慢，于是想给服务器配个代理</p><h3 id="代理安装"><a href="#代理安装" class="headerlink" title="代理安装"></a>代理安装</h3><ol><li><p>配置pip源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; /root/.pip/pip.conf &lt;&lt; eof<br>[global]<br>trusted-host =  pypi.douban.com<br>index-url = http://pypi.douban.com/simple<br>eof<br></code></pre></td></tr></table></figure></li><li><p>pip升个级</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum install python3 -y &amp;&amp; pip3 install --upgrade pip<br></code></pre></td></tr></table></figure></li><li><p>安装ss</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install shadowsocks<br></code></pre></td></tr></table></figure></li><li><p>创建ss-local配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir /etc/ss<br>cat &gt; /etc/ss/ss.json &lt;&lt; eof<br>&#123;<br>    &quot;server&quot;:&quot;ss 服务端 ip&quot;,<br>    &quot;server_port&quot;:&quot;ss 服务端端口&quot;,<br>    &quot;local_address&quot;: &quot;127.0.0.1&quot;,<br>    &quot;local_port&quot;:1080,<br>    &quot;password&quot;:&quot;ss 服务端密码&quot;,<br>    &quot;timeout&quot;:300,<br>    &quot;method&quot;:&quot;aes-256-cfb&quot;,<br>    &quot;fast_open&quot;: false,<br>    &quot;workers&quot;: 1<br>&#125;<br>eof<br></code></pre></td></tr></table></figure></li><li><p>创建ss service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; /etc/systemd/system/ss.service &lt;&lt; eof<br>[Unit]<br>Description=ss<br>[Service]<br>TimeoutStartSec=0<br>Restart=always<br>RestartSec=30<br>ExecStart=/usr/local/bin/sslocal -c /etc/ss/ss.json start<br>ExecStop=/usr/bin/killall sslocal<br>[Install]<br>WantedBy=multi-user.target<br>eof<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 如果没有 killall ，则执行</span><br>yum install psmisc -y<br></code></pre></td></tr></table></figure></li><li><p>自启动ss service</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl daemon-reload<br>systemctl enable ss.service<br>systemctl start ss.service<br>systemctl status ss<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 到此为止，已经可以使用 ss 代理了，验证一下</span><br>curl --socks5 127.0.0.1:1080 http://httpbin.org/ip<br></code></pre></td></tr></table></figure></li><li><p>安装privoxy (为了使用 http 代理)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum install privoxy -y<br></code></pre></td></tr></table></figure></li><li><p>增加 privoxy 的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 增加一条转发规则</span><br>echo &#x27;forward-socks5t   /               127.0.0.1:1080 .&#x27; &gt;&gt; /etc/privoxy/config<br><span class="hljs-meta">#</span><span class="bash"> 默认配置已经打开 listen-address  127.0.0.1:8118 (http代理端口)</span><br></code></pre></td></tr></table></figure></li><li><p>启动 privoxy 代理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl enable privoxy<br>systemctl restart privoxy<br>systemctl status privoxy<br></code></pre></td></tr></table></figure></li><li><p>安装 proxychains-ng (为了支持单个进程的代理)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum install -y git<br>export http_proxy=http://127.0.0.1:8118; export https_proxy=https://127.0.0.1:8118; # 解决 git 慢的问题<br>git clone --depth=1 https://github.com/rofl0r/proxychains-ng<br>yum install gcc -y<br>cd proxychains-ng<br>./configure --prefix=/usr --sysconfdir=/etc<br>make &amp;&amp; make install &amp;&amp; make install-config<br></code></pre></td></tr></table></figure></li><li><p>修改 proxychains 配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/proxychains.conf<br><span class="hljs-meta">#</span><span class="bash"> 把最后一行的 socks4  127.0.0.1 9050 改成 socks5 127.0.0.1 1080</span><br></code></pre></td></tr></table></figure></li><li><p>起个别名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ln -s /usr/bin/proxychains4 /usr/bin/proxy<br></code></pre></td></tr></table></figure></li><li><p>测试一下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">proxy curl www.google.com<br></code></pre></td></tr></table></figure></li></ol><p>自此以后，如果没法访问一些资源，则使用：<br>proxy + 要执行的命令</p><p>如果想在整个终端中使用 http 代理，则： (这是 privoxy 带来的)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">export http_proxy=127.0.0.1:8118<br>export https_proxy=127.0.0.1:8118<br></code></pre></td></tr></table></figure><p>以下，终于可以不受限制地安装helm了。。。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">helm安装</span><br>wget https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz <br><br>tar -zxvf helm-v3.2.4-linux-amd64.tar.gz<br><br>mv linux-amd64/helm /usr/local/bin/helm<br></code></pre></td></tr></table></figure><p>相关的程序也可以看下： polipo</p><h3 id="其他-ss-问题"><a href="#其他-ss-问题" class="headerlink" title="其他 ss 问题"></a>其他 ss 问题</h3><ol><li><p>如果希望使用多级 ss (一台机器 作为 另一台机器的 中转 ss)</p><ul><li>能够使用 ssh， 则参照 <a href="/longblog/posts/21_12_11_reverse_proxy_compares.html" name="反穿技术哪家强" >反穿技术哪家强</a></li><li>能够使用 ss-tunnel，则 ① 中转机启动 ss-local (假设本地代理端口为 50000) ② 中转机启动 ss-server ③ 使用 <code>ss-tunnel -l 1080 -b 127.0.0.1 -L 127.0.0.1:50000</code> (监听本地 1080)。这种方案 和 ssh 的方案本质一样，都是转发本地端口到远端端口。</li><li>仅能使用 ss-local，则 ① 中转机启动 ss-local, 用于连接到上位ss  ② 中转机启动 ss-server, 用于承接 PC 上的连接  ③ 使用 proxy ssserver </li><li>如果中转机上使用的 ss-local , 那么任何能提供 <code>认证</code> 能力的 tunnel，都能满足需求，比如 <a href="https://github.com/ginuerzh/gost">gost</a>、<a href="https://github.com/ehang-io/nps">nps</a>、<a href="https://github.com/Dreamacro/clash">clash</a></li><li>最简单的方式，是直接用 iptables 转发即可，直接使用 <code>iptables -t nat -A PREROUTING -4 -p tcp --dport 50000 -j DNAT --to-destination xxx.xxx.xxx.xx:50001</code> 即可 (本地 50000 转发到 上位 ss 的 50001)<br>ps: 使用 iptables 有两个坑要注意下，① 要配置 内核参数允许转发 <code>net.ipv4.ip_forward = 1</code>  ② 要设置出网 ip 替换 <code>iptables -t nat -A POSTROUTING -4 -p tcp --dport 50001 -j MASQUERADE</code>，否则回包的地址就错了</li></ul></li><li><p>如果想用 ss 作为全局代理，可以使用 <code>redsocks + iptables</code> 的方案。</p></li><li><p>ssh 其实非常强大, 我们完全可以不用 ss-server 作为代理，直接使用 <code>ssh -D 1080 user@host</code>, 就能做到同样的事情。</p></li><li><p>调试网络的过程中，有几个很好用的工具</p><ul><li><code>ss</code> 查看 socket 相关信息，和 netstat 有重合的部分</li><li><code>netstat</code> 查看 socket 相关信息、路由信息</li><li><code>ip</code> 查看或修改 路由、网卡</li><li><code>ifconfig</code> 网卡信息查看及配置</li><li><code>tcpdump</code> tcp 抓包工具</li><li><code>telnet</code> 远程登录</li><li><code>nc</code> socket 转发</li><li><code>iptables</code> iptables 管理工具</li><li><code>ipvsadm</code> ipvs 管理工具</li><li><code>dig</code> / <code>nslookup</code> dns 查询</li><li><code>curl</code> 支持多种应用层协议的工具</li></ul></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>proxy</tag>
      
      <tag>ssh</tag>
      
      <tag>helm</tag>
      
      <tag>proxychains</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>反穿技术哪家强</title>
    <link href="/longblog/posts/21_12_11_reverse_proxy_compares.html"/>
    <url>/longblog/posts/21_12_11_reverse_proxy_compares.html</url>
    
    <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>有时候，因为种种原因，我们希望我们的机器能被其他一些原本访问不到我们的设备访问。<br>比如: </p><ol><li>做微信开发调试的时候，需要配置回调地址，也就是我们能访问微信的服务器，但微信的服务器需要访问我们时缺不行。</li><li>我们想要分享一些资料、文件、服务给不在同一个局域网的人。</li></ol><p>这些情况下，我们需要的技术方案，被称为 <code>内网穿透</code>。</p><h3 id="反穿可选方案"><a href="#反穿可选方案" class="headerlink" title="反穿可选方案"></a>反穿可选方案</h3><p>社区中，有很多这类工具。</p><h4 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h4><p>最简单的就是 ssh 的方案，只要开启了sshd，本地有ssh就能够实现，既然都在玩服务器了，又有几个没有 ssh 的呢。<br>使用起来非常简单：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> -R 和 -L 的差别只是翻转了，本身实现的效果是一样的</span><br>ssh -R 0.0.0.0:8081:0.0.0.0:8081 root@xx.xx.xx.xx<br></code></pre></td></tr></table></figure><p>上面这样，就把服务器的 8081 端口 代理到了本地的 8081。这个方案最大的优点就是 <code>简单</code> ，在调试时使用十分轻便。<br>问题有两个： 1. 不够稳定，容易断掉。 2. 只支持端口映射，更复杂的功能不好实现。</p><p>另外，ssh 其实是可以直接实现 socks 代理的，这样的目的在于，仅使用一个端口，就能实现对所有端口的访问：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> -f 为后台运行 -N 为不运行任何命令 -D 为动态代理  -q 为 quiet 模式</span><br>ssh  -qfND 0.0.0.0:1080 root@xxx.xx.xx.xx<br></code></pre></td></tr></table></figure><h4 id="autossh"><a href="#autossh" class="headerlink" title="autossh"></a>autossh</h4><p>对于ssh方案而言，最难搞的问题还是 <code>不稳定</code>，于是可以用 autossh 的方案解决。使用起来和 ssh 类似，不过多了一个监听端口：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">autossh -M 8082 -fNR 0.0.0.0:8081:0.0.0.0:8081 root@xx.xx.xx.xx<br></code></pre></td></tr></table></figure><p>这个方案最大的优势就是 解决了 ssh 断线重连的问题。因此，在开发调试时更加好用，而且 <code>-f</code> 参数支持了后台运行，比较优雅地解决了需要维持终端持续打开的问题。</p><p>基本上，在有一些临时性的穿透需求时，这个方案已经十分够用了。对于复杂功能的需求实际上不是这个工具要解决的问题，例如 udp、http、端口复用 等等。</p><h4 id="frp"><a href="#frp" class="headerlink" title="frp"></a>frp</h4><p>上面说的两种方案，主要是临时使用比较方便，但在功能的丰富度上还是十分不足的，毕竟 ssh 的主业是做加密登录的，端口映射只是副业。 而 frp 是则专门做内网穿透的。<br>如果希望提供比较稳定的内网穿透服务，那么选择 frp 就是比较好的选择了。</p><p>frp 分为 服务端 和 客户端，需要两端部署。通过 <code>.ini</code> 配置文件进行管理。支持 tcp、udp、http、ssl-tcp、ssl-udp、p2p 的配置，还对 http 代理做了一些优化(类似于 nginx 的一些功能)。</p><p>关于frp，官方文档介绍的肯定比我详细，可以参考 <a href="https://gofrp.org/docs/">frp文档</a></p><h4 id="nps"><a href="#nps" class="headerlink" title="nps"></a>nps</h4><p>nps 和 frp 是同样定位的应用，两者在功能上也几乎一致，不过值得一提的是，nps自带官方的可视化管理页面，这点比frp在易用性上增加了不少(当然frp也有几个简单的社区版可视化工具)。</p><p>同样，详细可以参考 <a href="https://ehang-io.github.io/nps/">nps文档</a></p><p>实际上，类似的工具还挺多，在 github 上还扒到另一个项目： <a href="https://github.com/mmatczuk/go-http-tunnel">https://github.com/mmatczuk/go-http-tunnel</a> , 再比如和 nps 类似的 <a href="https://github.com/inconshreveable/ngrok">ngrok</a> (ps: 这个项目1.0是开源的，2.0搞成商业项目了，然后1.0就不开发了，有nps代替基本可以不用关注这个项目了)</p><h3 id="一些其他的想法"><a href="#一些其他的想法" class="headerlink" title="一些其他的想法"></a>一些其他的想法</h3><p>上面说的 内网穿透，是解决网络通路的其中一种场景，除了上面这些方案，还有一些其他的方向，举些栗子：</p><h4 id="NAT映射"><a href="#NAT映射" class="headerlink" title="NAT映射"></a>NAT映射</h4><p>这种解决网络通路的方案，通常用在 <code>网关</code> 处，可以根据 网卡、ip、端口 做响应的转发。</p><h4 id="反向代理"><a href="#反向代理" class="headerlink" title="反向代理"></a>反向代理</h4><p>反向代理最常见的就是充当 <code>应用网关</code>，通过对 协议、域名、路径、header 等等做匹配，转发到内网中的不同服务上去。对于提供网络应用的软件来说，这种方式几乎是必然的。</p><p>传统应用中，最广泛的反向代理服务器就是 <code>nginx</code>。在云原生逐渐成长起来之后，ambassador、envoy、kong 等应用网关也顺势起飞了。</p><h4 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h4><p>对大众而言，最常接触到的解决网络通路的场景，还是 <code>代理</code>，为了和反向代理区分，我们不妨叫它 <code>正向代理</code>。</p><p>使用正向代理的场景很明确： 当我们的机器 A 无法访问机器 C，但是另一台机器 B 能访问机器 C，同时我们的机器 A 能访问机器 B，那么，机器 A 就可以让机器 B 代理 A 的请求，访问机器 C。</p><p>大众最常见的例子是，在国内，想要用手机看油管的视频，就需要安装代理软件。这个场景中，手机就是 机器A，油管的服务器就是机器 C，代理软件连接的服务端就是机器 B。</p><p>举个开发场景的例子：<br>  公司开发环境的服务器A在公司内网，并且做了网络白名单，仅有特殊的网段能够访问，你的电脑不在这个特殊网段内，有一台机器B就在这个网段，你能访问这台机器 B，那么你就能通过机器 B 访问开发环境的服务器 A。通常，这个机器 B 可能是一个 <code>跳板机</code>，或者是一个 <code>vpn</code>。</p><p>我们通常认为，代理是为了解决 <code>网络不可达</code> 问题的，有些时候，代理也为了解决 <code>不那么可达</code> 的问题 ，俗称 <code>网速慢</code>。</p><p>比如在玩游戏这件事上，一些国外服的游戏，在国内玩起来就很慢，类似的还有看视频等场景。这种时候的代理，都是为了 <code>加速</code>，往往会采用 <code>udp</code> 的方案，可以参考 <a href="https://github.com/wangyu-/UDPspeeder/blob/branch_libev/doc/README.zh-cn.md">UDPspeeder</a></p><p>如果要自己建立网络代理服务，最常见的方案是 <code>ss</code> 、<code>open VPN</code> , 这相关的资料网络上十分丰富，就不多说。<br>在 mac/windows/iphone/android 上安装代理客户端比较简单，但是在 linux 上安装代理客户端就相对不那么常见了，之前为了安装 helm，解决过在服务器上安装客户端的问题，详情可参考 <a href="/longblog/posts/21_12_12_how_to_resolve_proxy_in_linux.html" name="如何解决linux下的代理访问" >如何解决linux下的代理访问</a></p><h3 id="网络相关的内容"><a href="#网络相关的内容" class="headerlink" title="网络相关的内容"></a>网络相关的内容</h3><p>网络连通性还有一些其他方面的，平常可能用得比较少。</p><h4 id="p2p"><a href="#p2p" class="headerlink" title="p2p"></a>p2p</h4><p>是 <code>peer to peer</code> 的缩写，现在最多的应用场景是 <code>p2p内容分发网络</code>，比如大家说的 <code>种子下载</code>。另外，在区块链上，也使用了相同的技术。另一种比较大众的应用场景，就是 <code>语音电话</code>、<code>视频电话</code> 。</p><p>p2p 用在私有领域，可以用来作为 安全通信 的技术方案，例如私有部署的即时通信。除了通信，还可以作为安全文件传输的方案，相关信息可以参考 <a href="https://magic-wormhole.readthedocs.io/en/latest/welcome.html">magic-wormhole</a></p><h4 id="k8s-开发网络代理"><a href="#k8s-开发网络代理" class="headerlink" title="k8s 开发网络代理"></a>k8s 开发网络代理</h4><p>一般我们认为，要访问 k8s 中的服务，有这样几种方案：</p><ol><li>nodeport</li><li>loadbalancer</li><li>ingress</li></ol><p>在开发调试阶段，我们有时候想直接访问 k8s 中的服务，但是这个服务在 k8s 中仅有 ClusterIP，单独为这个服务创建 LB 也划不来，这时候可以使用 <code>kubectl port-forward [podname] [local port]:[pod port]</code>，如果遇到需要 port-forward 的数量比较多的时候，这种方式就比较麻烦了，此时需要批量转发，可以参考 <a href="https://github.com/txn2/kubefwd">kubefwd</a>，也可以参考可视化的项目 <a href="https://github.com/pixel-point/kube-forwarder">kube-forwarder</a></p><p>除了想在本地直接访问集群中的服务，有时候我们也想把集群中某个服务所收到的流量，转发到我们本地端口，这样就可以实现方便地调试。</p><p>我们目前使用的方案是 devspace，具体信息可以查看 <a href="https://devspace.sh/cli/docs/introduction">devspace</a></p><p>其他可以考虑的方案可以查看</p><ul><li><a href="https://github.com/telepresenceio/telepresence">telepresence</a></li><li><a href="https://github.com/omrikiei/ktunnel">ktunnel</a></li><li><a href="https://nocalhost.dev/zh-CN/">nocalhost</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>proxy</tag>
      
      <tag>ssh</tag>
      
      <tag>frp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>websocket百万连接测试</title>
    <link href="/longblog/posts/21_12_02_load_test_of_million_websocket_conns.html"/>
    <url>/longblog/posts/21_12_02_load_test_of_million_websocket_conns.html</url>
    
    <content type="html"><![CDATA[<h2 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h2><p>websocket作为一个与客户端直接连接通信的中间人，在大用户量的情况下，需要建立大量的连接，在此测试单台机器进行websocket的连接具有多大的潜力。</p><h3 id="测试目的"><a href="#测试目的" class="headerlink" title="测试目的"></a>测试目的</h3><ol><li>机器的连接数潜力</li><li>机器的网络传输潜力</li></ol><h3 id="测试内容"><a href="#测试内容" class="headerlink" title="测试内容"></a>测试内容</h3><ol><li>单台机器进行连接数测试</li><li>单台机器进行传输量测试</li></ol><h3 id="硬件资源："><a href="#硬件资源：" class="headerlink" title="硬件资源："></a>硬件资源：</h3><ol><li>被测试机  ecs 4c8g * 1</li><li>压测机 ecs 4c8g * 2</li></ol><h3 id="软件准备："><a href="#软件准备：" class="headerlink" title="软件准备："></a>软件准备：</h3><ol><li>websocket server 端 (基于 gorilla 写的简单服务)</li><li>websocket client 端 (基于 gorilla 写的简单客户端)</li></ol><h2 id="连接测试"><a href="#连接测试" class="headerlink" title="连接测试"></a>连接测试</h2><h3 id="开始运行server端并监控数据"><a href="#开始运行server端并监控数据" class="headerlink" title="开始运行server端并监控数据"></a>开始运行server端并监控数据</h3><p><img src="https://static.longalong.cn/img/1.png" alt="1"></p><h3 id="遇到问题1"><a href="#遇到问题1" class="headerlink" title="遇到问题1"></a>遇到问题1</h3><p>压测量到达50000左右时上不去了<br><img src="https://static.longalong.cn/img/2.png" alt="2"></p><p>发现客户端报错：<br><img src="https://static.longalong.cn/img/3.png" alt="3"></p><p>查看系统内核参数中对 port 的分配：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ws_press_client_02 ~]# sysctl -a  |grep port_range<br>net.ipv4.ip_local_port_range = 32768        60999<br></code></pre></td></tr></table></figure><p>发现仅有28000个左右，要调整。<br>解决办法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 1. 调低端口释放后的等待时间，默认为60s，修改为15~30s</span><br>sysctl -w net.ipv4.tcp_fin_timeout=15<br><span class="hljs-meta">#</span><span class="bash"> 2. 修改tcp/ip协议配置， 通过配置/proc/sys/net/ipv4/tcp_tw_resue, 默认为0，修改为1，释放TIME_WAIT端口给新连接使用</span><br>sysctl -w net.ipv4.tcp_timestamps=1<br><span class="hljs-meta">#</span><span class="bash"> 3. 修改tcp/ip协议配置，快速回收socket资源，默认为0，修改为1</span><br>sysctl -w net.ipv4.tcp_tw_recycle=1<br><span class="hljs-meta">#</span><span class="bash"> 4. 增加可用端口</span><br>sysctl -w net.ipv4.ip_local_port_range =&quot;2000  65000&quot;<br></code></pre></td></tr></table></figure><h3 id="遇到问题2"><a href="#遇到问题2" class="headerlink" title="遇到问题2"></a>遇到问题2</h3><p>客户端报错：<br><img src="https://static.longalong.cn/img/4.png" alt="4"></p><p>服务端报错：<br><img src="https://static.longalong.cn/img/5.png" alt="5"></p><p>问题是</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ws_press_server ~]# ulimit -a<br>core file size          (blocks, -c) 0<br>data seg size           (kbytes, -d) unlimited<br>scheduling priority             (-e) 0<br>file size               (blocks, -f) 1024<br>pending signals                 (-i) 31202<br>max locked memory       (kbytes, -l) 64<br>max memory size         (kbytes, -m) unlimited<br>open files                      (-n) 65535  =&gt; 文件句柄大小限制<br>pipe size            (512 bytes, -p) 8<br>POSIX message queues     (bytes, -q) 819200<br>real-time priority              (-r) 0<br>stack size              (kbytes, -s) 8192<br>cpu time               (seconds, -t) unlimited<br>max user processes              (-u) 31202<br>virtual memory          (kbytes, -v) unlimited<br>file locks                      (-x) unlimited<br></code></pre></td></tr></table></figure><p>解决办法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ws_press_server ~]# ulimit -n 1000000<br>[root@ws_press_server ~]# ulimit -a<br>core file size          (blocks, -c) 0<br>data seg size           (kbytes, -d) unlimited<br>scheduling priority             (-e) 0<br>file size               (blocks, -f) 1024<br>pending signals                 (-i) 31202<br>max locked memory       (kbytes, -l) 64<br>max memory size         (kbytes, -m) unlimited<br>open files                      (-n) 1000000<br>pipe size            (512 bytes, -p) 8<br>POSIX message queues     (bytes, -q) 819200<br>real-time priority              (-r) 0<br>stack size              (kbytes, -s) 8192<br>cpu time               (seconds, -t) unlimited<br>max user processes              (-u) 31202<br>virtual memory          (kbytes, -v) unlimited<br>file locks                      (-x) unlimited<br></code></pre></td></tr></table></figure><h3 id="第一波结果"><a href="#第一波结果" class="headerlink" title="第一波结果"></a>第一波结果</h3><p>到现在为止，达到120000连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@ws_press_server ~]# echo &amp;&amp; curl localhost:8080/conns &amp;&amp; echo &amp;&amp; echo<br><br>&#123;&quot;closed&quot;:0,&quot;conns&quot;:120000&#125;<br></code></pre></td></tr></table></figure><p>查看server端资源耗用<br><img src="https://static.longalong.cn/img/6.png" alt="6"><br>Ps: cpu的变化较大，从40%到400%不等。（由于数据发送的周期性）</p><p>client端资源耗用<br><img src="https://static.longalong.cn/img/7.png" alt="7"></p><p>单个连接资源耗用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">&gt;</span><span class="bash"> 8 * 1024 * 1024 // 总kb数</span><br>8388608<br><span class="hljs-meta">&gt;</span><span class="bash"> 8 * 1024 * 1024 * 36.7 * 0.01 // 占用kb数</span><br>3078619<br><span class="hljs-meta">&gt;</span><span class="bash"> 3078619 / 120000 // 单个连接占用kb数</span><br>25.65<br></code></pre></td></tr></table></figure><h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><p>目前的瓶颈在压测服务器的port资源上，因此，如果想达到 1,000,000 的数量，需要 63000(连接) * 16(机器)  = 1008000 。<br>从ws服务器的资源使用情况来看，4c8g是无法支撑这个量的，按照每个连接需要约 26kb 的内存来算，需要至少 26 kb * 1000000 = 25 g 的内存。</p><h3 id="第二波测试"><a href="#第二波测试" class="headerlink" title="第二波测试"></a>第二波测试</h3><h4 id="换配置"><a href="#换配置" class="headerlink" title="换配置"></a>换配置</h4><p>服务端： 16c32g * 1<br>压测端： 2c4g * 16</p><p>斥巨资买服务器，纪念一下<br><img src="https://static.longalong.cn/img/8.png" alt="8"></p><p>执行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">./main -addr 172.16.0.42:8080 -con 63000 -time 180 -path /ws -co 1000 -wait 180<br></code></pre></td></tr></table></figure><h3 id="执行情况"><a href="#执行情况" class="headerlink" title="执行情况"></a>执行情况</h3><p><img src="https://static.longalong.cn/img/9.png" alt="9"><br><img src="https://static.longalong.cn/img/10.png" alt="10"><br>达到了单机100万连接，然后进程就被杀死了……</p><h3 id="结论1"><a href="#结论1" class="headerlink" title="结论1"></a>结论1</h3><p>基本算是达到了单机百万连接的目标。<br>但实际上，连接量并不是我们所追求的，而应当是在有数据传输下的表现，因此要对数据传输进行压测。</p><h2 id="数据传输压测"><a href="#数据传输压测" class="headerlink" title="数据传输压测"></a>数据传输压测</h2><h3 id="第一轮：-4-10000-连接，-100byte每100ms"><a href="#第一轮：-4-10000-连接，-100byte每100ms" class="headerlink" title="第一轮： 4 * 10000 连接， 100byte每100ms"></a>第一轮： 4 * 10000 连接， 100byte每100ms</h3><p>结果：<br><img src="https://static.longalong.cn/img/11.png" alt="11"><br><img src="https://static.longalong.cn/img/12.png" alt="12"></p><p>60s内总共发送了 2344 MB 数据，cpu使用大约为 960%</p><h3 id="第二轮：-4-10000-连接，-50byte每50ms"><a href="#第二轮：-4-10000-连接，-50byte每50ms" class="headerlink" title="第二轮： 4 * 10000 连接， 50byte每50ms"></a>第二轮： 4 * 10000 连接， 50byte每50ms</h3><p><img src="https://static.longalong.cn/img/13.png" alt="13"></p><h3 id="第三轮：-4-10000-连接，-200byte每100ms"><a href="#第三轮：-4-10000-连接，-200byte每100ms" class="headerlink" title="第三轮： 4 * 10000 连接， 200byte每100ms"></a>第三轮： 4 * 10000 连接， 200byte每100ms</h3><p><img src="https://static.longalong.cn/img/14.png" alt="14"></p><h3 id="第四轮：-4-10000-连接，-1000byte每100ms"><a href="#第四轮：-4-10000-连接，-1000byte每100ms" class="headerlink" title="第四轮： 4 * 10000 连接， 1000byte每100ms"></a>第四轮： 4 * 10000 连接， 1000byte每100ms</h3><p><img src="https://static.longalong.cn/img/15.png" alt="15"></p><h3 id="第五轮：-4-5000-连接，-2000byte每100ms"><a href="#第五轮：-4-5000-连接，-2000byte每100ms" class="headerlink" title="第五轮： 4 * 5000 连接， 2000byte每100ms"></a>第五轮： 4 * 5000 连接， 2000byte每100ms</h3><p><img src="https://static.longalong.cn/img/16.png" alt="16"></p><h3 id="第六轮：-4-20000-连接，-1000byte每100ms"><a href="#第六轮：-4-20000-连接，-1000byte每100ms" class="headerlink" title="第六轮： 4 * 20000 连接， 1000byte每100ms"></a>第六轮： 4 * 20000 连接， 1000byte每100ms</h3><p><img src="https://static.longalong.cn/img/17.png" alt="17"></p><h3 id="第七轮：-4-20000-连接，-100byte每200ms"><a href="#第七轮：-4-20000-连接，-100byte每200ms" class="headerlink" title="第七轮： 4 * 20000 连接， 100byte每200ms"></a>第七轮： 4 * 20000 连接， 100byte每200ms</h3><p><img src="https://static.longalong.cn/img/18.png" alt="18"></p><h3 id="第八轮：-4-20000-连接，-50byte每100ms"><a href="#第八轮：-4-20000-连接，-50byte每100ms" class="headerlink" title="第八轮： 4 * 20000 连接， 50byte每100ms"></a>第八轮： 4 * 20000 连接， 50byte每100ms</h3><p><img src="https://static.longalong.cn/img/19.png" alt="19"></p><h3 id="第九轮：-4-20000-连接，-100byte每100ms"><a href="#第九轮：-4-20000-连接，-100byte每100ms" class="headerlink" title="第九轮： 4 * 20000 连接， 100byte每100ms"></a>第九轮： 4 * 20000 连接， 100byte每100ms</h3><p><img src="https://static.longalong.cn/img/20.png" alt="20"></p><h3 id="第十轮：-4-20000-连接，-1000byte每200ms-64-k连接·kb-ms"><a href="#第十轮：-4-20000-连接，-1000byte每200ms-64-k连接·kb-ms" class="headerlink" title="第十轮： 4 * 20000 连接， 1000byte每200ms = 64 k连接·kb/ms"></a>第十轮： 4 * 20000 连接， 1000byte每200ms = 64 k连接·kb/ms</h3><p><img src="https://static.longalong.cn/img/21.png" alt="21"></p><h3 id="第十一轮：-4-20000-连接，-2000byte每400ms-64-k连接-·-kb-ms"><a href="#第十一轮：-4-20000-连接，-2000byte每400ms-64-k连接-·-kb-ms" class="headerlink" title="第十一轮： 4 * 20000 连接， 2000byte每400ms  = 64 k连接 · kb/ms"></a>第十一轮： 4 * 20000 连接， 2000byte每400ms  = 64 k连接 · kb/ms</h3><p><img src="https://static.longalong.cn/img/22.png" alt="22"></p><h3 id="第十二轮：-4-20000-连接，-1000-byte-每-200ms"><a href="#第十二轮：-4-20000-连接，-1000-byte-每-200ms" class="headerlink" title="第十二轮： 4 * 20000 连接， 1000 byte 每 200ms"></a>第十二轮： 4 * 20000 连接， 1000 byte 每 200ms</h3><p><img src="https://static.longalong.cn/img/23.png" alt="23"></p><h3 id="第十三轮：-4-25000-连接，-1000-byte-每-200ms-大量报错"><a href="#第十三轮：-4-25000-连接，-1000-byte-每-200ms-大量报错" class="headerlink" title="第十三轮： 4 * 25000 连接， 1000 byte 每 200ms (大量报错)"></a>第十三轮： 4 * 25000 连接， 1000 byte 每 200ms (大量报错)</h3><p><img src="https://static.longalong.cn/img/24.png" alt="24"></p><h3 id="执行命令"><a href="#执行命令" class="headerlink" title="执行命令"></a>执行命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">./main -addr 172.16.0.42:8080 -con 20000 -time 180 -path /ws -co 1000 -wait 180 -sendtick 100 -databyte 100<br></code></pre></td></tr></table></figure><h3 id="结论2"><a href="#结论2" class="headerlink" title="结论2"></a>结论2</h3><ol><li>关注平均负载是很重要的</li><li>对内存耗用不高，对cpu耗用非常高</li><li>单次传输量对cpu的影响不大(估计是和 socket 的读写缓冲大小相关)</li><li>传输频次对cpu的影响较大</li></ol><h2 id="其他文档"><a href="#其他文档" class="headerlink" title="其他文档"></a>其他文档</h2><p>不错的总结： <a href="https://blog.csdn.net/Fredric_2014/article/details/89019815">《使用Go实现支持百万连接的websocket服务器》笔记(上)</a><br>不错的排查： <a href="https://blog.csdn.net/c359719435/article/details/80300433">记一次压测问题定位:connection reset by peer，TCP三次握手后服务端发送RST</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>websocket</tag>
      
      <tag>load test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>穷人才会用的nas方案</title>
    <link href="/longblog/posts/21_12_02_a_nas_solution_used_by_poor.html"/>
    <url>/longblog/posts/21_12_02_a_nas_solution_used_by_poor.html</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近手机开始报存储空间快满了的问题，不知道有些什么文件占用了这么多空间，基本的解决办法是：把我将来需要的文件进行转移。例如之前的照片、视频等。自然而然想到了文件服务器。</p><h2 id="可选方案"><a href="#可选方案" class="headerlink" title="可选方案"></a>可选方案</h2><h3 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h3><p>采用现有的公有网盘，例如 百度网盘，阿里网盘，方便直接。<br>问题： 百度网盘太慢了，会员又太贵了，划不来。阿里网盘现在的客户端还很不完善。另外，可能还会有资料隐私问题、莫名关停服务等问题(参考360网盘)。<br>结论：对于一些公共资料可以采用这种方案，例如一些课程学习资料等。等阿里网盘发育一段时间再存一些公共资料。</p><h3 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h3><p>采用OSS，自建私有网盘，保证安全私密性。<br>问题：贵。oss存储费用 + 流量费，对于可能需要长期存储的文件来说，成本太高了。<br>结论：不行不行，穷。</p><h3 id="方案三"><a href="#方案三" class="headerlink" title="方案三"></a>方案三</h3><p>采用自有硬盘，搭建文件服务器，成本低，私密性高。<br>问题：</p><ol><li> 搭建和维护相对复杂。</li><li>存储安全性需要考虑。</li></ol><p>由于成本问题以及私密性问题，最终采用方案三。</p><h2 id="具体可选实施方案"><a href="#具体可选实施方案" class="headerlink" title="具体可选实施方案"></a>具体可选实施方案</h2><h3 id="购买成熟系统"><a href="#购买成熟系统" class="headerlink" title="购买成熟系统"></a>购买成熟系统</h3><p>现在市场上关于 家庭NAS 系统，主要有 威联通、群晖、铁威马 这些。<br>优点就是 简单、方便、功能齐全。缺点就是 价格贵！<br><img src="https://static.longalong.cn/img/tieweima_nas_price.png" alt="群晖、威联通价格"></p><p><img src="https://static.longalong.cn/img/xishu_nas_price.png" alt="西部数据价格"></p><p>其实直接购买的方式优势还是非常明显的，一个群晖，不仅可以作为文件服务器，还可以用作下载机、家庭影院、简单办公系统，由于其本质是一个linux系统的封装，因此也可以跑很多的服务(例如docker)，类似于买了一台家用服务器，而文件存储服务只是这个服务器的一个小服务而已。</p><h3 id="使用塔式-家用主机"><a href="#使用塔式-家用主机" class="headerlink" title="使用塔式/家用主机"></a>使用塔式/家用主机</h3><p>和直接购买群晖这类机器类似的另一种方案，就是直接使用家用主机。<br>家用主机的好处就是，大部分家庭中都有现有的设备，也就不需要单独花钱了，多配一块硬盘就可以了。</p><h3 id="使用树莓派"><a href="#使用树莓派" class="headerlink" title="使用树莓派"></a>使用树莓派</h3><p>使用主机作为文件服务器有一个弊端，就是能耗太高了，一个家用主机至少也是80W以上，常年开机的话，也不是一个环保的方式(主要可能还是成本太高)。<br>如果使用树莓派的话，则更加轻量，树莓派的运行功耗仅 5W ，放在某个小角落，一年下来问题也不大。<br>另外，树莓派可以提供的服务也可以非常非常多，甚至可以用来做智能家居控制器。</p><h3 id="使用路由器"><a href="#使用路由器" class="headerlink" title="使用路由器"></a>使用路由器</h3><p>在考虑树莓派的网络拓扑时，想到了为何不直接使用路由器作为文件服务器？<br>由于路由器的内核也是类unix系统，因此也是可以直接作为文件服务器的。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>考虑到成本，最终选择使用 路由器 作为文件服务器的处理器。</p><h2 id="具体操作方案"><a href="#具体操作方案" class="headerlink" title="具体操作方案"></a>具体操作方案</h2><h3 id="购买硬盘架、硬盘"><a href="#购买硬盘架、硬盘" class="headerlink" title="购买硬盘架、硬盘"></a>购买硬盘架、硬盘</h3><p>要将路由器作为文件服务器，首先就要挂载硬盘。同时，考虑到文件安全的问题，需要采用冗余存储的方式，因此，需要至少2块硬盘。<br>要达到目标，有两种方案：</p><ul><li>使用usb hub + usb 转接口 + 串口硬盘</li><li>使用usb hub + usb接口磁盘</li></ul><p>由于usb接口的磁盘价格都比较高(例如u盘、ssd等)，因此选择第一种方案。<br>去淘宝搜了一下，一个usb hub大约16元，一个usb转接口大约27元 * 2，一个硬盘大约60元(500g) * 2，成本为190元。但这样有一个问题，用usb转接口的话，线太多了，得有两个12v的供电线以及2个sata转usb数据线，而且硬盘还是散乱的。<br>最终一狠心，花了150块大洋买了个双盘位硬盘架，这样就省去了usb hub和usb转接口的钱，最终算下来花了 270元。</p><h3 id="安装同步软件"><a href="#安装同步软件" class="headerlink" title="安装同步软件"></a>安装同步软件</h3><p>软件方面，有syncthing，非常合适，拥有多个操作系统的客户端。<br>参考官网： Syncthing</p><h3 id="连接同步"><a href="#连接同步" class="headerlink" title="连接同步"></a>连接同步</h3><p>路由器上和手机上都安装了syncthing后，进行两端绑定并共享文件夹。<br>然后设置同步策略为每小时扫描一次。</p><h3 id="啥也不用管了"><a href="#啥也不用管了" class="headerlink" title="啥也不用管了"></a>啥也不用管了</h3><p>然后就去睡觉吧，保证syncthing后台进程，你就几个月都不用管他，如果哪天手机空间不足了，就去看看是否最新同步的，如果是，就把手机上的文件都清空吧。</p><h2 id="PS-回头补一些图片"><a href="#PS-回头补一些图片" class="headerlink" title="PS: 回头补一些图片"></a>PS: 回头补一些图片</h2>]]></content>
    
    
    
    <tags>
      
      <tag>nas</tag>
      
      <tag>syncthing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用lvm实现raid能力</title>
    <link href="/longblog/posts/21_12_02_operation_of_raid_with_lvm.html"/>
    <url>/longblog/posts/21_12_02_operation_of_raid_with_lvm.html</url>
    
    <content type="html"><![CDATA[<h3 id="lvm是什么？"><a href="#lvm是什么？" class="headerlink" title="lvm是什么？"></a>lvm是什么？</h3><p>LVM 是 Logical Volume Manager（逻辑卷管理）的简写，它是Linux环境下对磁盘分区进行管理的一种机制，它由Heinz Mauelshagen在Linux 2.4内核上实现，最新版本为：稳定版1.0.5，开发版 1.1.0-rc2，以及LVM2开发版。Linux用户安装Linux操作系统时遇到的一个常见的难以决定的问题就是如何正确地评估各分区大小，以分配合适的硬盘空间。普通的磁盘分区管理方式在逻辑分区划分好之后就无法改变其大小，当一个逻辑分区存放不下某个文件时，这个文件因为受上层文件系统的限制，也不能跨越多个分区来存放，所以也不能同时放到别的磁盘上。而遇到出现某个分区空间耗尽时，解决的方法通常是使用符号链接，或者使用调整分区大小的工具，但这只是暂时解决办法，没有从根本上解决问题。随着Linux的逻辑卷管理功能的出现，这些问题都迎刃而解，用户在无需停机的情况下可以方便地调整各个分区大小。</p><h3 id="实验操作"><a href="#实验操作" class="headerlink" title="实验操作"></a>实验操作</h3><p>创建设备<br><img src="./../static/img/ali_ecs_lvm.png" alt="阿里云ecs"></p><h4 id="查看磁盘状态"><a href="#查看磁盘状态" class="headerlink" title="查看磁盘状态"></a>查看磁盘状态</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# fdisk -l<br><br>磁盘 /dev/vda：21.5 GB, 21474836480 字节，41943040 个扇区<br>Units = 扇区 of 1 * 512 = 512 bytes<br>扇区大小(逻辑/物理)：512 字节 / 512 字节<br>I/O 大小(最小/最佳)：512 字节 / 512 字节<br>磁盘标签类型：dos<br>磁盘标识符：0x000bb9c1<br><br>   设备 Boot      Start         End      Blocks   Id  System<br>/dev/vda1   *        2048    41943039    20970496   83  Linux<br><br>磁盘 /dev/vdb：21.5 GB, 21474836480 字节，41943040 个扇区<br>Units = 扇区 of 1 * 512 = 512 bytes<br>扇区大小(逻辑/物理)：512 字节 / 512 字节<br>I/O 大小(最小/最佳)：512 字节 / 512 字节<br><br><br>磁盘 /dev/vdc：21.5 GB, 21474836480 字节，41943040 个扇区<br>Units = 扇区 of 1 * 512 = 512 bytes<br>扇区大小(逻辑/物理)：512 字节 / 512 字节<br>I/O 大小(最小/最佳)：512 字节 / 512 字节<br><br><br>磁盘 /dev/vdd：21.5 GB, 21474836480 字节，41943040 个扇区<br>Units = 扇区 of 1 * 512 = 512 bytes<br>扇区大小(逻辑/物理)：512 字节 / 512 字节<br>I/O 大小(最小/最佳)：512 字节 / 512 字节<br></code></pre></td></tr></table></figure><h4 id="升级yum"><a href="#升级yum" class="headerlink" title="升级yum"></a>升级yum</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# yum update -y<br></code></pre></td></tr></table></figure><h4 id="安装lvm2"><a href="#安装lvm2" class="headerlink" title="安装lvm2"></a>安装lvm2</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# yum install lvm2 -y<br></code></pre></td></tr></table></figure><h4 id="创建物理卷"><a href="#创建物理卷" class="headerlink" title="创建物理卷"></a>创建物理卷</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# pvcreate /dev/vdb /dev/vdc<br>  Physical volume &quot;/dev/vdb&quot; successfully created.<br>  Physical volume &quot;/dev/vdc&quot; successfully created.<br></code></pre></td></tr></table></figure><h4 id="查看物理卷"><a href="#查看物理卷" class="headerlink" title="查看物理卷"></a>查看物理卷</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# pvdisplay<br>  &quot;/dev/vdc&quot; is a new physical volume of &quot;20.00 GiB&quot;<br>  --- NEW Physical volume ---<br>  PV Name               /dev/vdc<br>  VG Name<br>  PV Size               20.00 GiB<br>  Allocatable           NO<br>  PE Size               0<br>  Total PE              0<br>  Free PE               0<br>  Allocated PE          0<br>  PV UUID               avPba4-Z0J5-bhWC-4poS-x5oG-qt4P-4xUJnn<br><br>  &quot;/dev/vdb&quot; is a new physical volume of &quot;20.00 GiB&quot;<br>  --- NEW Physical volume ---<br>  PV Name               /dev/vdb<br>  VG Name<br>  PV Size               20.00 GiB<br>  Allocatable           NO<br>  PE Size               0<br>  Total PE              0<br>  Free PE               0<br>  Allocated PE          0<br>  PV UUID               exRn9W-AQKP-7rTc-EwAf-YtsK-MW0O-HHJ0sk<br></code></pre></td></tr></table></figure><h4 id="查看物理卷简要信息"><a href="#查看物理卷简要信息" class="headerlink" title="查看物理卷简要信息"></a>查看物理卷简要信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# pvs<br>  PV         VG Fmt  Attr PSize  PFree<br>  /dev/vdb      lvm2 ---  20.00g 20.00g<br>  /dev/vdc      lvm2 ---  20.00g 20.00g<br></code></pre></td></tr></table></figure><h4 id="查看物理卷简要信息-1"><a href="#查看物理卷简要信息-1" class="headerlink" title="查看物理卷简要信息"></a>查看物理卷简要信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# pvscan<br>  PV /dev/vdc                      lvm2 [20.00 GiB]<br>  PV /dev/vdb                      lvm2 [20.00 GiB]<br>  Total: 2 [40.00 GiB] / in use: 0 [0   ] / in no VG: 2 [40.00 GiB]<br></code></pre></td></tr></table></figure><h4 id="创建逻辑卷组"><a href="#创建逻辑卷组" class="headerlink" title="创建逻辑卷组"></a>创建逻辑卷组</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# vgcreate vglong /dev/vdc /dev/vdb<br>  Volume group &quot;vglong&quot; successfully created<br></code></pre></td></tr></table></figure><p>查看逻辑卷组信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# vgdisplay<br>  --- Volume group ---<br>  VG Name               vglong<br>  System ID<br>  Format                lvm2<br>  Metadata Areas        2<br>  Metadata Sequence No  1<br>  VG Access             read/write<br>  VG Status             resizable<br>  MAX LV                0<br>  Cur LV                0<br>  Open LV               0<br>  Max PV                0<br>  Cur PV                2<br>  Act PV                2<br>  VG Size               39.99 GiB<br>  PE Size               4.00 MiB<br>  Total PE              10238<br>  Alloc PE / Size       0 / 0<br>  Free  PE / Size       10238 / 39.99 GiB<br>  VG UUID               4flIRX-1oVR-MBy5-I2YT-gY2R-Eqaf-PmFq7b<br></code></pre></td></tr></table></figure><h4 id="查看逻辑卷组简要信息"><a href="#查看逻辑卷组简要信息" class="headerlink" title="查看逻辑卷组简要信息"></a>查看逻辑卷组简要信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# vgs<br>  VG     #PV #LV #SN Attr   VSize  VFree<br>  vglong   2   0   0 wz--n- 39.99g 39.99g<br></code></pre></td></tr></table></figure><h4 id="创建逻辑卷"><a href="#创建逻辑卷" class="headerlink" title="创建逻辑卷"></a>创建逻辑卷</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# lvcreate -L 30G -n lvlong vglong<br>  Logical volume &quot;lvlong&quot; created.<br></code></pre></td></tr></table></figure><h4 id="查看逻辑卷"><a href="#查看逻辑卷" class="headerlink" title="查看逻辑卷"></a>查看逻辑卷</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# lvdisplay<br>  --- Logical volume ---<br>  LV Path                /dev/vglong/lvlong<br>  LV Name                lvlong<br>  VG Name                vglong<br>  LV UUID                x7iFjg-QSem-vzfu-jAkV-V0lI-50P3-tHD3Hz<br>  LV Write Access        read/write<br>  LV Creation host, time longtestlvm, 2021-10-12 14:00:09 +0800<br>  LV Status              available<br><span class="hljs-meta">  #</span><span class="bash"> open                 0</span><br>  LV Size                30.00 GiB<br>  Current LE             7680<br>  Segments               2<br>  Allocation             inherit<br>  Read ahead sectors     auto<br>  - currently set to     8192<br>  Block device           252:0<br></code></pre></td></tr></table></figure><h4 id="查看逻辑卷简要信息"><a href="#查看逻辑卷简要信息" class="headerlink" title="查看逻辑卷简要信息"></a>查看逻辑卷简要信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# lvs<br>  LV     VG     Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert<br>  lvlong vglong -wi-a----- 30.00g<br></code></pre></td></tr></table></figure><h4 id="查看磁盘"><a href="#查看磁盘" class="headerlink" title="查看磁盘"></a>查看磁盘</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# fdisk -l<br><br>磁盘 /dev/vda：21.5 GB, 21474836480 字节，41943040 个扇区<br>Units = 扇区 of 1 * 512 = 512 bytes<br>扇区大小(逻辑/物理)：512 字节 / 512 字节<br>I/O 大小(最小/最佳)：512 字节 / 512 字节<br>磁盘标签类型：dos<br>磁盘标识符：0x000bb9c1<br><br>   设备 Boot      Start         End      Blocks   Id  System<br>/dev/vda1   *        2048    41943039    20970496   83  Linux<br><br>磁盘 /dev/vdb：21.5 GB, 21474836480 字节，41943040 个扇区<br>Units = 扇区 of 1 * 512 = 512 bytes<br>扇区大小(逻辑/物理)：512 字节 / 512 字节<br>I/O 大小(最小/最佳)：512 字节 / 512 字节<br><br><br>磁盘 /dev/vdc：21.5 GB, 21474836480 字节，41943040 个扇区<br>Units = 扇区 of 1 * 512 = 512 bytes<br>扇区大小(逻辑/物理)：512 字节 / 512 字节<br>I/O 大小(最小/最佳)：512 字节 / 512 字节<br><br><br>磁盘 /dev/vdd：21.5 GB, 21474836480 字节，41943040 个扇区<br>Units = 扇区 of 1 * 512 = 512 bytes<br>扇区大小(逻辑/物理)：512 字节 / 512 字节<br>I/O 大小(最小/最佳)：512 字节 / 512 字节<br><br><br>磁盘 /dev/mapper/vglong-lvlong：32.2 GB, 32212254720 字节，62914560 个扇区<br>Units = 扇区 of 1 * 512 = 512 bytes<br>扇区大小(逻辑/物理)：512 字节 / 512 字节<br>I/O 大小(最小/最佳)：512 字节 / 512 字节<br></code></pre></td></tr></table></figure><h4 id="格式化虚拟磁盘"><a href="#格式化虚拟磁盘" class="headerlink" title="格式化虚拟磁盘"></a>格式化虚拟磁盘</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# mkfs.ext4 /dev/vglong/lvlong<br>mke2fs 1.42.9 (28-Dec-2013)<br>文件系统标签=<br>OS type: Linux<br>块大小=4096 (log=2)<br>分块大小=4096 (log=2)<br>Stride=0 blocks, Stripe width=0 blocks<br>1966080 inodes, 7864320 blocks<br>393216 blocks (5.00%) reserved for the super user<br>第一个数据块=0<br>Maximum filesystem blocks=2155872256<br>240 block groups<br>32768 blocks per group, 32768 fragments per group<br>8192 inodes per group<br>Superblock backups stored on blocks:<br>        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,<br>        4096000<br><br>Allocating group tables: 完成<br>正在写入inode表: 完成<br>Creating journal (32768 blocks): 完成<br>Writing superblocks and filesystem accounting information: 完成<br></code></pre></td></tr></table></figure><h4 id="挂载磁盘"><a href="#挂载磁盘" class="headerlink" title="挂载磁盘"></a>挂载磁盘</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm mnt]# mkdir longtestlvm<br><br>[root@longtestlvm mnt]# mount /dev/mapper/vglong-lvlong /mnt/longtestlvm/<br></code></pre></td></tr></table></figure><h4 id="创建一个文件试试"><a href="#创建一个文件试试" class="headerlink" title="创建一个文件试试"></a>创建一个文件试试</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm longtestlvm]# mkdir -p /mnt/longtestlvm/longtest<br><br>[root@longtestlvm longtestlvm]# echo &#x27;hello world \n nihao a&#x27; &gt; /mnt/longtestlvm/longtest/a.txt<br></code></pre></td></tr></table></figure><h4 id="查看磁盘挂载"><a href="#查看磁盘挂载" class="headerlink" title="查看磁盘挂载"></a>查看磁盘挂载</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm mnt]# df -h<br>文件系统                   容量  已用  可用 已用% 挂载点<br>devtmpfs                   3.9G     0  3.9G    0% /dev<br>tmpfs                      3.9G     0  3.9G    0% /dev/shm<br>tmpfs                      3.9G  572K  3.9G    1% /run<br>tmpfs                      3.9G     0  3.9G    0% /sys/fs/cgroup<br>/dev/vda1                   20G  2.4G   17G   13% /<br>tmpfs                      783M     0  783M    0% /run/user/0<br>/dev/mapper/vglong-lvlong   30G   45M   28G    1% /mnt/longtestlvm<br></code></pre></td></tr></table></figure><h4 id="查看文件大小"><a href="#查看文件大小" class="headerlink" title="查看文件大小"></a>查看文件大小</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm mnt]# du -ah<br>4.0K        ./longtestlvm/longtest/a.txt<br>8.0K        ./longtestlvm/longtest<br>16K        ./longtestlvm/lost+found<br>28K        ./longtestlvm<br>32K        .<br></code></pre></td></tr></table></figure><h4 id="扩展逻辑卷容量"><a href="#扩展逻辑卷容量" class="headerlink" title="扩展逻辑卷容量"></a>扩展逻辑卷容量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# lvextend -L +4G /dev/vglong/lvlong<br>  Size of logical volume vglong/lvlong changed from 30.00 GiB (7680 extents) to 34.00 GiB (8704 extents).<br>  Logical volume vglong/lvlong successfully resized.<br></code></pre></td></tr></table></figure><h4 id="resize逻辑卷"><a href="#resize逻辑卷" class="headerlink" title="resize逻辑卷"></a>resize逻辑卷</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# resize2fs /dev/vglong/lvlong<br>resize2fs 1.42.9 (28-Dec-2013)<br>Filesystem at /dev/vglong/lvlong is mounted on /mnt/longtestlvm; on-line resizing required<br>old_desc_blocks = 4, new_desc_blocks = 5<br>The filesystem on /dev/vglong/lvlong is now 8912896 blocks long.<br></code></pre></td></tr></table></figure><h4 id="扩展逻辑卷组"><a href="#扩展逻辑卷组" class="headerlink" title="扩展逻辑卷组"></a>扩展逻辑卷组</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# vgextend vglong /dev/vdd<br>  Physical volume &quot;/dev/vdd&quot; successfully created.<br>  Volume group &quot;vglong&quot; successfully extended<br></code></pre></td></tr></table></figure><h4 id="缩减逻辑卷"><a href="#缩减逻辑卷" class="headerlink" title="缩减逻辑卷"></a>缩减逻辑卷</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# umount /mnt/longtestlvm<br><br>[root@longtestlvm ~]# e2fsck -f /dev/vglong/lvlong<br>e2fsck 1.42.9 (28-Dec-2013)<br>第一步: 检查inode,块,和大小<br>第二步: 检查目录结构<br>第3步: 检查目录连接性<br>Pass 4: Checking reference counts<br>第5步: 检查簇概要信息<br>/dev/vglong/lvlong: 13/2228224 files (0.0% non-contiguous), 184932/8912896 blocks<br><br><br>[root@longtestlvm ~]#  resize2fs /dev/vglong/lvlong  10G<br>resize2fs 1.42.9 (28-Dec-2013)<br>Resizing the filesystem on /dev/vglong/lvlong to 2621440 (4k) blocks.<br>The filesystem on /dev/vglong/lvlong is now 2621440 blocks long.<br><br><br>[root@longtestlvm ~]# lvresize -L 10G /dev/vglong/lvlong<br>  WARNING: Reducing active logical volume to 10.00 GiB.<br>  THIS MAY DESTROY YOUR DATA (filesystem etc.)<br>Do you really want to reduce vglong/lvlong? [y/n]: y<br>  Size of logical volume vglong/lvlong changed from 34.00 GiB (8704 extents) to 10.00 GiB (2560 extents).<br>  Logical volume vglong/lvlong successfully resized.<br></code></pre></td></tr></table></figure><h4 id="缩减逻辑卷组"><a href="#缩减逻辑卷组" class="headerlink" title="缩减逻辑卷组"></a>缩减逻辑卷组</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# vgreduce vglong /dev/vdd<br></code></pre></td></tr></table></figure><h4 id="查看逻辑卷组"><a href="#查看逻辑卷组" class="headerlink" title="查看逻辑卷组"></a>查看逻辑卷组</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# vgdisplay<br>  --- Volume group ---<br>  VG Name               vglong<br>  System ID<br>  Format                lvm2<br>  Metadata Areas        2<br>  Metadata Sequence No  10<br>  VG Access             read/write<br>  VG Status             resizable<br>  MAX LV                0<br>  Cur LV                1<br>  Open LV               0<br>  Max PV                0<br>  Cur PV                2<br>  Act PV                2<br>  VG Size               39.99 GiB<br>  PE Size               4.00 MiB<br>  Total PE              10238<br>  Alloc PE / Size       2560 / 10.00 GiB<br>  Free  PE / Size       7678 / 29.99 GiB<br>  VG UUID               4flIRX-1oVR-MBy5-I2YT-gY2R-Eqaf-PmFq7b<br></code></pre></td></tr></table></figure><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# lvremove /dev/vglong/lvlong<br>Do you really want to remove active logical volume vglong/lvlong? [y/n]: y<br>  Logical volume &quot;lvlong&quot; successfully removed<br><br>[root@longtestlvm ~]# vgremove vglong<br>  Volume group &quot;vglong&quot; successfully removed<br>  <br>[root@longtestlvm ~]# pvremove /dev/vdb /dev/vdc /dev/vdd<br>  Labels on physical volume &quot;/dev/vdb&quot; successfully wiped.<br>  Labels on physical volume &quot;/dev/vdc&quot; successfully wiped.<br>  Labels on physical volume &quot;/dev/vdd&quot; successfully wiped.<br></code></pre></td></tr></table></figure><h4 id="迁移"><a href="#迁移" class="headerlink" title="迁移"></a>迁移</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# pvmove /dev/vdb /dev/vdc<br></code></pre></td></tr></table></figure><h4 id="一些其他的内容"><a href="#一些其他的内容" class="headerlink" title="一些其他的内容"></a>一些其他的内容</h4><p>disk相关命令<br>fdisk 、 mkfs 、du 、 df 、dump2fs</p><h4 id="一些问题记录"><a href="#一些问题记录" class="headerlink" title="一些问题记录"></a>一些问题记录</h4><ol><li>已经将某个物理磁盘加入到 vg 中后，再将该物理盘进行分区<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# pvs<br>  WARNING: Device for PV cSASFg-dS4S-lrIP-hD3V-aJSR-EnzC-5bYp4B not found or rejected by a filter.<br>  Couldn&#x27;t find device with uuid cSASFg-dS4S-lrIP-hD3V-aJSR-EnzC-5bYp4B.<br>  PV         VG     Fmt  Attr PSize   PFree<br>  /dev/vdb   vglong lvm2 a--  &lt;20.00g &lt;20.00g<br>  /dev/vdc   vglong lvm2 a--  &lt;20.00g &lt;20.00g<br>  /dev/vdd1         lvm2 ---  100.00m 100.00m<br>  /dev/vdd2         lvm2 ---  100.00m 100.00m<br>  [unknown]  vglong lvm2 a-m  &lt;20.00g &lt;20.00g<br></code></pre></td></tr></table></figure></li></ol><p>解决办法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@longtestlvm ~]# vgreduce --removemissing vglong --force<br>  Volume group &quot;vglong&quot; is already consistent.<br>  <br><br>[root@longtestlvm ~]# pvs<br>  PV         VG     Fmt  Attr PSize   PFree<br>  /dev/vdb   vglong lvm2 a--  &lt;20.00g &lt;20.00g<br>  /dev/vdc   vglong lvm2 a--  &lt;20.00g &lt;20.00g<br>  /dev/vdd1         lvm2 ---  100.00m 100.00m<br>  /dev/vdd2         lvm2 ---  100.00m 100.00m<br></code></pre></td></tr></table></figure><h4 id="整体的一些想法"><a href="#整体的一些想法" class="headerlink" title="整体的一些想法"></a>整体的一些想法</h4><p>对于 lvm，可以用于多个磁盘组合成一个磁盘，且提供 raid 能力，这给存储提供了超大容量的可能性，可以考虑作为NAS的一种实现。</p>]]></content>
    
    
    
    <tags>
      
      <tag>raid</tag>
      
      <tag>lvm</tag>
      
      <tag>disk</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何做好压测</title>
    <link href="/longblog/posts/21_12_02_how_to_do_load_test.html"/>
    <url>/longblog/posts/21_12_02_how_to_do_load_test.html</url>
    
    <content type="html"><![CDATA[<h3 id="压测的目标是什么？"><a href="#压测的目标是什么？" class="headerlink" title="压测的目标是什么？"></a>压测的目标是什么？</h3><p>压测的目标有两个：</p><ol><li>通过对模块的压测，找到模块的性能瓶颈，分析其资源耗用合理性、架构可扩展性，找到优化方向。</li><li>通过对链路的压测，找到链路的性能瓶颈，回答线上资源容量。 ^20d5ad</li></ol><h3 id="压测的技能栈有什么？"><a href="#压测的技能栈有什么？" class="headerlink" title="压测的技能栈有什么？"></a>压测的技能栈有什么？</h3><p>压测的技能栈要求非常广泛，整体来说，需要具备这些技能：</p><ol><li>测试人员的思维习惯和方法论。找到合适的测试模型，并准备好/管理好这些数据。</li><li>运维人员的对系统的认识。能够根据需要配置合适的系统资源，能够一定程度上进行系统问题排查。</li><li>开发人员的开发能力。往往在数据准备过程、测试过程中，需要有较多的工具类开发。</li><li>数据分析人员的分析能力。在数据准备和压测过程中，需要提前预规划各类指标与数据，用于分析系统状态，有时需要比较强的数据分析能力。</li><li>对业务系统熟悉。能够在一定程度上进行问题排查。</li><li>对常用的中间件熟悉。能够进行常规中间件的性能评估与异常分析，例如 <code>kafka</code>、<code>mongodb</code>、<code>mysql</code>、<code>redis</code> 等。</li></ol><p>具体的技能，需要根据业务情况而定。但也有一些通用的技能：</p><ol><li>工作规划能力。由于压测的工作往往十分庞杂，因此要能规划好工作内容，各个阶段的安排要合理。</li><li>文档书写能力。压测的过程十分庞杂，因此每个阶段的工作都应该有比较好的记录和分析，用于指导下一环节的工作。常写的文档包括： 1.  压测规划书  2. 压测结果记录   3. 压测分析报告  4. 系统结构分析图。</li><li>沟通能力。由于压测涉及到的范围很广，尤其是一个比较大的链路压测时，压测人员基本无法对整个系统有较强的掌控能力，需要 1. 市场/运营人员  2. 模块开发负责人员  3. 运维人员  的参与，因此协调这些人员的工作就会十分重要。</li></ol><h3 id="压测的形态是怎样的？"><a href="#压测的形态是怎样的？" class="headerlink" title="压测的形态是怎样的？"></a>压测的形态是怎样的？</h3><ol><li>压测可以分为基准测试和链路测试。基准测试是对单个模块的压测。链路压测是对一条业务流程的压测。</li><li>对于一个常规的压测流程，应当先对模块进行基准测试，在得到每个模块的性能指标后，再进行链路压测。</li></ol><h3 id="压测的结论有哪些？"><a href="#压测的结论有哪些？" class="headerlink" title="压测的结论有哪些？"></a>压测的结论有哪些？</h3><ol><li>回答压测的模型是怎样的，说明为什么选择这样的压测模型</li><li>回答在目标QPS下，当前的系统瓶颈是什么</li><li>给出压测过程中发现的不合理之处</li></ol><h3 id="压测的流程是怎样的？"><a href="#压测的流程是怎样的？" class="headerlink" title="压测的流程是怎样的？"></a>压测的流程是怎样的？</h3><ol><li>分析业务形态，系统形态</li><li>根据业务需求，确定合适的压测模型</li><li>准备压测环境</li><li>准备压测数据、压测工具</li><li>确定系统可观测性，确定测试终止条件</li><li>压测执行，处理过程中的问题</li><li>根据压测过程，形成压测报告</li><li>根据压测报告进行分析，形成分析报告</li></ol><h3 id="压测的工程化实践"><a href="#压测的工程化实践" class="headerlink" title="压测的工程化实践"></a>压测的工程化实践</h3><p>业务系统的压测(从链路来看)，往往可以分为这两类：</p><ol><li>http 请求</li><li>其他类 TCP 请求</li></ol><p>由于业务系统主要面向用户，因此对于 http 请求的压测是最多的，这也是市场上生态最好的类型。常规的例如 <code>jmeter</code>、<code>locust</code>、<code>k6</code>、<code>PTS</code>、<code>loadrunner</code> ，这是用于系统性压测的工具。另外，在开发人员保障的性能测试中，<code>ab</code> 、<code>wrk</code> 、<code>go-stress-testing</code>  这几个工具被经常使用，用于快速验证。<br>其他类的请求相对较少，例如 <code>rpc</code>、<code>websocket</code> 这类。即使有一些实现，但真正结合到业务中时，会发现各种不满足需求。实际上，这类测试最好还是自己根据业务情况写项目实现。<br>压测项目是一个比较具有通用性的项目，因此可以考虑将其平台化，但目前来看，在最通用的 http 压测上，是比较容易平台化的，市场上也有很多，例如上面提到的 <code>jmeter</code>、<code>loadrunner</code> 等。不过，市面上的这类工具基本都是用来进行 压力管理 的，而测试计划、测试脚本、测试报告 等方面的管理是比较弱的，因此，也有企业在开始做云端化的压测管理平台，比如 <code>metersphere</code>，现在做得也基本可用了。</p><p>对于接口类的测试，通用性是非常强的，例如和 接口管理 结合，可以实现类似于：<code>自动化生成测试用例</code>、<code>流量录制</code>、<code>测试跟踪(bug管理等)</code>、<code>基于接口测试的性能测试</code>等等。实际上，这些功能 <code>metersphere</code> 也支持了，不过有部分是企业版的(如项目管理平台集成)。<br>对于接口类测试，我们当前使用的是自己开发的一个项目 <code>super-apitest</code> ，这个项目的最大亮点在于 基于json的模板化用例 ，这点可以认为和 jmeter 的 jmx 格式类比，不过基于 json 的格式，对开发者更加友好。<br>现在这个项目仅有一些简单的功能，例如适配了简单的触发界面、结果报告、结果通知，但想要真正做到平台化，还有非常长的路要走。<br>当前来看，这个项目可以在一定程度上进行平台化，主要的功能点有 3 个：</p><ol><li>支持所有用例使用数据库管理 =&gt; 一切演进的前提</li><li>支持测试用例录制 =&gt; 极大简化用例生成方式</li><li>支持测试用例通过率统计 =&gt; 极大增强测试跟踪</li></ol><p>如果做到这 3 点，这个接口测试平台已经基本可用了。</p><p>对于其他类型的功能测试，主要包括</p><ol><li>工具库的单测和覆盖度测试</li><li>模块的功能测试</li><li>非 http 类的接口测试 (例如 rpc、ws、异步消息)</li></ol><p>这些测试基本不具备很好的通用性，平台化的效率比较低，对于这类测试，平台的作用其实主要有:</p><ol><li>测试触发</li><li>测试用例元信息管理</li><li>测试结果展示</li></ol><p>基于此，可以认为，平台仅需要提供特定的接口，由各测试项目自行实现测试触发、测试结果反馈即可。</p><p>对于性能测试，如果全为 http 类接口，则平台化的效率较高，可以直接和 接口测试 模块相结合，由接口测试提供用例，由性能测试模块提供压测控制。<br>同样，对于非 http 类接口而言，性能测试的通用性就比较弱了，比如各个项目的性能。那么平台依然可以提供测试管理接口，由平台来做触发和结果采集，实际的测试执行由各业务项目自行处理。</p><h3 id="功能测试和性能测试的关系"><a href="#功能测试和性能测试的关系" class="headerlink" title="功能测试和性能测试的关系"></a>功能测试和性能测试的关系</h3><p>功能测试的目的在于保证功能的正确性，有时会有比较复杂的校验逻辑，测试用例集的组织形式经常为一个功能。<br>性能测试的目的在于摸清一些功能或场景的最大(合适)并发度，用于排查性能瓶颈和做线上资源容量规划，校验逻辑往往比较轻量，着重在压力 ，测试用例集的组织形式尝尝是一系列场景，并且十分关注各场景的比例，目的模拟真实用户的请求。</p><h3 id="压测项目的价值有多大"><a href="#压测项目的价值有多大" class="headerlink" title="压测项目的价值有多大"></a>压测项目的价值有多大</h3><p>根据压测的两大目标来看： 1. 发现系统瓶颈，提供优化建议  2. 回答线上容量问题<br>对于第 1 点，这是项目开发时需要的，例如开发一个通用的库，需要根据压测的结果进行优化。实际上，这是任何一个开发工程师都需要具备的能力，尤其是写中间件的工程师。<br>这种情况下，压测基本上是驱动这个项目进化的源动力之一。也是业务方评判这个项目的优劣及适用性的一个很重要的标准，业界流行的 <code>redis</code>、<code>kafka</code>、<code>mongo</code>、<code>mysql</code>等这类中间件，都是直接在发行版中自带 benchmark 工具。<br>对打第 2 个问题，大多数时候需要链路压测。最常见的系统压测是基于 http 这类 “request/response” 的，而这也是测试工程师最常接触的。目标在于找到整个链路的性能峰值。<br>毫无疑问，在项目开发过程中，我们需要回答第 1 个问题，而往往这个问题需要研发工程师自己回答。但从实际的情况来看，对于大多数业务场景，这类问题是比较基础的问题，如果项目中有比较高级一些的研发工程师在技术评审时下点功夫，基本就能将这些问题扼杀在摇篮里。<br>也就意味着，业务系统做这类压测，原因往往有两个： 1. 技术管理体系不成熟，代码质量得不到保障。 2. 跟其他业务有所关联，需要自证。<br>如果要回答第 2 个问题，就需要链路压测，这也是最被大家所熟悉的压测，这类压测往往需要一个团队来执行，主要包含： 1. <code>测试工程师</code>  2. <code>运维工程师</code>  3. <code>研发工程师</code>  4. <code>业务人员(运营/产品)</code>。 在新项目上线、大促活动等情况下，一般需要做这类测试，用于保障线上不会被打垮。</p><h3 id="业界做的比较好的容量是怎样的"><a href="#业界做的比较好的容量是怎样的" class="headerlink" title="业界做的比较好的容量是怎样的"></a>业界做的比较好的容量是怎样的</h3><p>根据一些公开资料，有了解到使用机器学习的方式，得出各条件下对于机器资源的需求量。机器学习的来源，一方面是通过多次(很多)压测得到，另一方面，是通过线上实际数据反馈进行修正。<br>这样一套容量的机器学习方案确实是一个不错的选择，尤其是对于业务较多的企业，还是有不错的收益，例如阿里的本地生活就有这么一套。</p><h3 id="性能保障的职责"><a href="#性能保障的职责" class="headerlink" title="性能保障的职责"></a>性能保障的职责</h3><ol><li>由测试人员</li><li>由运维人员(sre)</li><li>由开发人员</li><li>由架构师</li></ol><p>对于常规的 http 请求的压测，可以由测试人员负责，凭借测试人员对业务场景的理解，可以构造出符合需求的测试脚本，再使用特定的压测工具，则可进行压测，压测的结果也比较清晰，可以直接交给相关项目的开发人员去进行接口优化。<br>针对特殊项目的压测，和普通意义上我们认为的测试工程师所做的工作不太一样，常规认为的测试工程师，大多凭借对业务场景的理解，加上对测试(功能/性能)工具的掌握，能做针对业务场景的测试/压测。这种项目的性能测试需要对这个项目本身比较熟悉，往往需要自行开发一些压测的工具来进行压测，从上述描述的测试工程师所擅长的技能来看，不能很好匹配。<br>对于链路压测，一般来说需要准备的内容比较复杂，包含 <code>压测目标确定</code>、<code>环境准备</code>、<code>压测场景构建</code>、<code>压测脚本</code>、<code>压测过程控制</code>、<code>项目配置/环境配置调整</code>、<code>压测资源监控</code>、<code>压测报告</code>、<code>压测结果分析</code> 等一系列环节。<br>这些环节过于复杂，对于任意一个工程师而言，都是几乎无法完成的。因此，往往需要一个团队来配合。团队内的分工大致如下：</p><ol><li>业务人员(产品/运营)，负责压测目标的确定(业务侧视角)。</li><li>测试人员，负责压测场景梳理、压测数据准备(协同研发人员)、压测执行、压测结果收集、压测报告</li><li>研发人员，负责standby问题排查、环境配置调整</li><li>运维人员，负责环境准备、协助排查资源问题</li><li>架构师/高级开发人员，负责压测结果报告分析、优化方案</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>load test</tag>
      
      <tag>压测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>梳理一下要做的重要的事</title>
    <link href="/longblog/posts/21_09_30_to_think_what_is_the_important_things.html"/>
    <url>/longblog/posts/21_09_30_to_think_what_is_the_important_things.html</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>时光飞逝，转眼就来到了季度的末尾，工作上，一般会在季末时进行季度总结，我觉得还挺有价值，自己也同样需要做这类季度总结，当然，更重要的，是梳理清楚自己下个季度有哪些重要的事要做。</p><h2 id="上个季度"><a href="#上个季度" class="headerlink" title="上个季度"></a>上个季度</h2><p>工作上，主要做私有化部署的整套内容，包含依赖摘除、工具链调研、环境搭建等诸多事，整体打分 60 分。</p><p><strong>【私有化】完成的事：</strong></p><ol><li>调研了多种私有化部署方案，例如 <code>k8s</code>、<code>micro kube</code>、<code>k3s</code> 等，最后基于多方面考虑，选择了 <code>k3s</code></li><li>调研了数个文件存储方案，例如 <code>glusterFS</code>、<code>minio</code>，最后选择了 <code>minio</code></li><li>为私有化部署开发了 <code>license server</code> 项目</li><li>部署脚本采用 <code>ansible</code> 管理方案</li></ol><p><strong>【私有化】未完成的事：</strong></p><ol><li>私有化的扩缩容方案</li><li>私有化的数据冷热备方案</li><li>私有化的升级迁移方案</li><li>私有化的权限控制方案</li><li>私有化后台[业务需求不明]</li></ol><p><strong>【质量与性能】完成的事：</strong></p><ol><li>调研多个性能压测工具，例如 <code>ab</code>、 <code>locust</code>、 <code>jmeter</code>、 <code>PTS</code> 、 <code>wrk</code>、<code>go-stress-testing</code>、<code>meter-sphere</code>、<code>k6</code> 等，由于主要是为 websocket 测试，未找到合适轮子，于是自己写，基于 <code>gorilla/websocket</code> 写的强业务耦合测试代码。</li><li>接口测试方面，接口测试框架经调研未发现好用的工具，主要调研了 <code>yapi</code>、 <code>postman</code>、 <code>swagger</code>、 <code>metersphere</code> ，于是依然使用自己造的接口测试轮子，改了部分 bug。</li><li>websocket 压测，阶段性瓶颈在于 <code>mongodb</code>，经部分调整，提升最大并发量 10 倍，瓶颈依然在 <code>mongodb</code>，下一阶段计划重新进行数据库选型。</li><li>代码质量方面，部分项目集成 <code>golangci-lint</code>，同时调研了多个代码质量保证方面的工具链，例如 <code>Covergates</code>、 <code>goc</code>、<code>gitlab pages</code> 等。</li></ol><p><strong>【质量与性能】未完成的事：</strong></p><ol><li>未能对压测进行系统性总结</li><li>未能对调研的各类工具做实操性文档</li><li>对于 <code>mongodb</code> 的性能瓶颈未能做更高层次的思考与总结</li><li>测试框架未能支持 rpc 测试</li><li>测试框架未能支持 延迟 操作</li><li>测试框架未能支持 form-data 支持</li><li>测试框架未能支持 数据库 存储</li><li>测试框架未能支持 录制 接口与自动化测试用例生成</li><li>代码质量保证还未推广到所有 golang 项目</li></ol><p><strong>【其他】完成的事：</strong></p><ol><li>完成了截图服务器开发和部署，为视觉感知测试铺了路</li><li>开始学习 计算机基础 类知识<ul><li>操作系统</li><li>计算机网络</li><li>序列化方案</li></ul></li><li>开始重新写博客</li></ol><p><strong>【其他】未完成的事：</strong></p><ol><li>视觉感知测试还没有做过比较细致的分析</li><li>计算机基础类知识没有记成笔记</li><li>博客内容规划还不够细致</li></ol><h2 id="这个季度"><a href="#这个季度" class="headerlink" title="这个季度"></a>这个季度</h2><p><strong>【质量与性能】要做的事：</strong></p><ol><li>充分调研 <code>nosql</code> 数据库, 结合业务需求，全面梳理适用场景<ul><li>mongodb</li><li>redis</li><li>groupcache</li><li>levelDB</li><li>cassandra</li><li>……</li></ul></li><li>形成基本完备的压测工具链体系<ul><li>解决 rpc 压测支持</li><li>解决 ws 压测自动化</li><li>解决 api 压测自动化</li><li>完成 接口测试 协议设计，最好能支持部分协议转换</li></ul></li><li>代码质量保证<ul><li>所有项目添加 代码静态检查 golangci-lint</li><li>工具库项目全部添加 测试覆盖度报告</li><li>所有有自动化测试的项目添加覆盖度 ci</li></ul></li><li>初步形成对第三方中间件的benchmark工具链<ul><li>redis</li><li>mongodb</li><li>mysql</li><li>kafka</li></ul></li></ol><p><strong>【私有化】要做的事：</strong></p><ol><li>私有化的扩缩容方案</li><li>私有化的数据冷热备方案</li><li>私有化的升级迁移方案</li><li>私有化的权限控制方案</li><li>私有化后台[业务需求不明]</li></ol><p><strong>【其他】要做的事：</strong></p><ol><li>把学习的内容进行梳理整合，并形成博客 * 30</li><li>刷 leetcode 题目 * 60</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>要做的事情非常多，因此每天的时间要做好规划，不能 <code>什么堆到眼前再做什么</code>，每天要做好当天的事情总结。<br>加油。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>收拾一下，重新出发</title>
    <link href="/longblog/posts/21_09_20_new_start.html"/>
    <url>/longblog/posts/21_09_20_new_start.html</url>
    
    <content type="html"><![CDATA[<h3 id="预"><a href="#预" class="headerlink" title="预"></a>预</h3><p>前两天，收到腾讯网站备案人员的电话，说我的网站底部没有添加备案号，让补一下，我这才想起自己的博客已经很多年没更新了=.=!。</p><p>早上很早就醒了，也不想看手边的那些大部头技术书，于是就打开了我的博客，浏览了一遍自己曾经写下的文字。没想到，看到有些文章时竟忍不住产生一丝丝感动，回想起了自己一路走来的历程：</p><h3 id="前缘"><a href="#前缘" class="headerlink" title="前缘"></a>前缘</h3><ul><li>毕业前的实习是做的审计，半年左右的审计经历让我意识到，我在财会这条路上，走不远</li><li>于是开始问自己，我喜欢什么？</li><li>摇摆很久，最终选定试一下 编程</li><li>毫无任何头绪地学习技术，从 C、单片机、arduino 开始玩，也不知道能做啥</li><li>接触了树莓派，艰难地在 linux 环境上挣扎</li><li>前端的所见即所得让我看到一丝光亮，开始学习 html、css、js、jq、vue</li><li>不知道自己学得咋样了，模仿写了 网易云课堂 和 B站</li><li>听了老爸的话: “不知道行不行没关系，去试试看，不试试永远不知道行不行”，鼓了鼓劲去面试</li></ul><h3 id="中上篇"><a href="#中上篇" class="headerlink" title="中上篇"></a>中上篇</h3><ul><li>去了 极客邦 做前端开发工程师。距毕业过去了 4 个月</li><li>学习前端的知识，主要是 vue 生态中的种种</li><li>js 熟练了些，就学 nodejs，mysql</li><li>工作之余写点前后端的项目玩一下，例如时间胶囊(一个web记事本)</li><li>觉得在审美上没啥天赋，考虑看看后端</li><li>迷茫，简单学了 python、java、lua，套开源框架写 demo，主要是 web 开发</li></ul><h3 id="中下篇"><a href="#中下篇" class="headerlink" title="中下篇"></a>中下篇</h3><ul><li>朋友做自媒体收益不错，想去挣点钱，就提了离职。此时，差一天就满半年了</li><li>自媒体一直没上路，没啥思路，在家挣扎了几个月，觉得自己可能积淀不够，还是需要先去历练</li><li>又重新评估了 java 和 python，最后选了 go，陆陆续续学了 2 个月</li><li>之前用 nodejs 和 python 写过小项目，但对真实的企业开发完全没概念，没信心</li><li>内心十分不安，在爱人的鼓励下，终于还是投出去了一些简历</li><li>去了蓝湖，做后端开发。此时，据离开上家公司已经 6 个月</li></ul><h3 id="梦初醒"><a href="#梦初醒" class="headerlink" title="梦初醒"></a>梦初醒</h3><ul><li>这么久以来，我的学习环境都是自己搭的，逐渐也有了些熟练度，渐渐对 linux 系统比较感兴趣</li><li>学习运维的东西，主要是 docker、k8s、ansible、shell 这些</li><li>对服务器有种奇妙的感觉，于是买了一台24核的Dell服务器，用于平常测试</li><li>团队中有些跟运维接触比较多的事，都是我在做，比如 私有化部署、性能测试、集成测试 等等</li><li>做开发以来，一直后悔自己大学几年没有学相关知识，跟科班的同学相比，我起步几乎晚了半个世纪</li><li>也认识到自己在计算机基础上需要补补，于是开始看一些书，主要是 网络、操作系统、计算机组成、编译原理 这些</li><li>不仅在基础上不足，在经验上也十分欠缺，于是开始看一些有点深度的知识，主要是一些厉害的开源项目的源码 这些</li></ul><h3 id="脚下的路"><a href="#脚下的路" class="headerlink" title="脚下的路"></a>脚下的路</h3><p>到现在，距离来蓝湖已经 10 个月了，时光飞逝，令人不住感慨，现在是中秋的前一天，印象中正是秋高气爽之时，然而北京这两日阴雨连绵，天色昏沉，不经意间，也生出了些惆怅的情绪……</p><p>以前博客写的很少，学东西都是靠在脑海里留下一些印象，时间稍微一长就容易忘记，这样不好。作为程序员，写博客其实应该是基本功，和 算法、数学、计算机原理、设计模式 这些是一个层次的，属于那种具有长远价值的东西，我也应当多下功夫。</p><p>我重开这个博客，目的就是把我在技术学习时的一些收获进行总结梳理，用技术文章的方式，把对应的知识加工输出，一来，可以加深自己对一些问题的认识，二来，如果这些文章也能有那么一丝的作用，让来我的小站逛逛的同学们感到有所收获，那也算是推进了社会进步，哈哈哈哈。</p><p>将来的一段时间，我会把精力主要放在 第 26 和 第 27 这两条内容上，因此博客会记录一些： 计算机网络、操作系统、编译原理、数据库原理、分布式架构、服务治理 等这些方面的内容，也会看情况分享一些读书笔记、源码阅读感受等内容。</p><p>最后，希望未来越来越好，我们。</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
